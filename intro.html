<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.1">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>1&nbsp; Your Deep Learning Journey – Practical Deep Learning for Coders</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./book2.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>


<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="1&nbsp; Your Deep Learning Journey – Practical Deep Learning for Coders">
<meta property="og:description" content="">
<meta property="og:image" content="https://fastai.github.io/fastbook2e/images/chapter7_neuron.png">
<meta property="og:site_name" content="Practical Deep Learning for Coders">
<meta property="og:image:height" content="376">
<meta property="og:image:width" content="602">
<meta name="twitter:title" content="1&nbsp; Your Deep Learning Journey – Practical Deep Learning for Coders">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="https://fastai.github.io/fastbook2e/images/chapter7_neuron.png">
<meta name="twitter:creator" content="jeremyphoward">
<meta name="twitter:site" content="fastdotai">
<meta name="twitter:image-height" content="376">
<meta name="twitter:image-width" content="602">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./intro.html">Getting Started</a></li><li class="breadcrumb-item"><a href="./intro.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Your Deep Learning Journey</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Practical Deep Learning for Coders</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Getting Started</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Your Deep Learning Journey</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./book2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title"><em>From Model to Production</em></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./book3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">AI and Data Ethics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">The Details</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mnist_basics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Under the Hood: Training a Digit Classifier</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./book5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title"><em>Image Classification</em></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./book6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title"><em>Other Computer Vision Problems</em></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./book7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title"><em>Training a State-of-the-Art Model</em></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./book8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title"><em>Collaborative Filtering Deep Dive</em></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./book9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title"><em>Tabular Modeling Deep Dive</em></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./book10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title"><em>NLP Deep Dive: RNNs</em></span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">From the Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./book11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title"><em>Data Munging with fastai’s Mid-Level API</em></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./book12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title"><em>A Language Model from Scratch</em></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./convolutions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./resnet.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">ResNets</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./book15.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title"><em>Application Architectures Deep Dive</em></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./accel_sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">The Training Process</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">A Neural Net from the Foundations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./book18.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title"><em>CNN Interpretation with CAM</em></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./book19.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title"><em>A fastai Learner from Scratch</em></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./book20.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title"><em>Concluding Thoughts</em></span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#deep-learning-is-for-everyone" id="toc-deep-learning-is-for-everyone" class="nav-link active" data-scroll-target="#deep-learning-is-for-everyone"><span class="header-section-number">1.1</span> Deep Learning Is for Everyone</a></li>
  <li><a href="#neural-networks-a-brief-history" id="toc-neural-networks-a-brief-history" class="nav-link" data-scroll-target="#neural-networks-a-brief-history"><span class="header-section-number">1.2</span> Neural Networks: A Brief History</a></li>
  <li><a href="#who-we-are" id="toc-who-we-are" class="nav-link" data-scroll-target="#who-we-are"><span class="header-section-number">1.3</span> Who We Are</a></li>
  <li><a href="#how-to-learn-deep-learning" id="toc-how-to-learn-deep-learning" class="nav-link" data-scroll-target="#how-to-learn-deep-learning"><span class="header-section-number">1.4</span> How to Learn Deep Learning</a>
  <ul class="collapse">
  <li><a href="#your-projects-and-your-mindset" id="toc-your-projects-and-your-mindset" class="nav-link" data-scroll-target="#your-projects-and-your-mindset">Your Projects and Your Mindset</a></li>
  </ul></li>
  <li><a href="#the-software-pytorch-fastai-and-jupyter" id="toc-the-software-pytorch-fastai-and-jupyter" class="nav-link" data-scroll-target="#the-software-pytorch-fastai-and-jupyter"><span class="header-section-number">1.5</span> The Software: PyTorch, fastai, and Jupyter</a></li>
  <li><a href="#your-first-model" id="toc-your-first-model" class="nav-link" data-scroll-target="#your-first-model"><span class="header-section-number">1.6</span> Your First Model</a>
  <ul class="collapse">
  <li><a href="#getting-a-gpu-deep-learning-server" id="toc-getting-a-gpu-deep-learning-server" class="nav-link" data-scroll-target="#getting-a-gpu-deep-learning-server">Getting a GPU Deep Learning Server</a></li>
  <li><a href="#running-your-first-notebook" id="toc-running-your-first-notebook" class="nav-link" data-scroll-target="#running-your-first-notebook">Running Your First Notebook</a></li>
  <li><a href="#sidebar-this-book-was-written-in-jupyter-notebooks" id="toc-sidebar-this-book-was-written-in-jupyter-notebooks" class="nav-link" data-scroll-target="#sidebar-this-book-was-written-in-jupyter-notebooks">Sidebar: This Book Was Written in Jupyter Notebooks</a></li>
  <li><a href="#end-sidebar" id="toc-end-sidebar" class="nav-link" data-scroll-target="#end-sidebar">End sidebar</a></li>
  <li><a href="#what-is-machine-learning" id="toc-what-is-machine-learning" class="nav-link" data-scroll-target="#what-is-machine-learning">What Is Machine Learning?</a></li>
  <li><a href="#what-is-a-neural-network" id="toc-what-is-a-neural-network" class="nav-link" data-scroll-target="#what-is-a-neural-network">What Is a Neural Network?</a></li>
  <li><a href="#a-bit-of-deep-learning-jargon" id="toc-a-bit-of-deep-learning-jargon" class="nav-link" data-scroll-target="#a-bit-of-deep-learning-jargon">A Bit of Deep Learning Jargon</a></li>
  <li><a href="#limitations-inherent-to-machine-learning" id="toc-limitations-inherent-to-machine-learning" class="nav-link" data-scroll-target="#limitations-inherent-to-machine-learning">Limitations Inherent To Machine Learning</a></li>
  <li><a href="#how-our-image-recognizer-works" id="toc-how-our-image-recognizer-works" class="nav-link" data-scroll-target="#how-our-image-recognizer-works">How Our Image Recognizer Works</a></li>
  <li><a href="#what-our-image-recognizer-learned" id="toc-what-our-image-recognizer-learned" class="nav-link" data-scroll-target="#what-our-image-recognizer-learned">What Our Image Recognizer Learned</a></li>
  <li><a href="#image-recognizers-can-tackle-non-image-tasks" id="toc-image-recognizers-can-tackle-non-image-tasks" class="nav-link" data-scroll-target="#image-recognizers-can-tackle-non-image-tasks">Image Recognizers Can Tackle Non-Image Tasks</a></li>
  <li><a href="#jargon-recap" id="toc-jargon-recap" class="nav-link" data-scroll-target="#jargon-recap">Jargon Recap</a></li>
  </ul></li>
  <li><a href="#deep-learning-is-not-just-for-image-classification" id="toc-deep-learning-is-not-just-for-image-classification" class="nav-link" data-scroll-target="#deep-learning-is-not-just-for-image-classification"><span class="header-section-number">1.7</span> Deep Learning Is Not Just for Image Classification</a>
  <ul class="collapse">
  <li><a href="#sidebar-the-order-matters" id="toc-sidebar-the-order-matters" class="nav-link" data-scroll-target="#sidebar-the-order-matters">Sidebar: The Order Matters</a></li>
  <li><a href="#end-sidebar-1" id="toc-end-sidebar-1" class="nav-link" data-scroll-target="#end-sidebar-1">End sidebar</a></li>
  <li><a href="#sidebar-datasets-food-for-models" id="toc-sidebar-datasets-food-for-models" class="nav-link" data-scroll-target="#sidebar-datasets-food-for-models">Sidebar: Datasets: Food for Models</a></li>
  <li><a href="#end-sidebar-2" id="toc-end-sidebar-2" class="nav-link" data-scroll-target="#end-sidebar-2">End sidebar</a></li>
  </ul></li>
  <li><a href="#validation-sets-and-test-sets" id="toc-validation-sets-and-test-sets" class="nav-link" data-scroll-target="#validation-sets-and-test-sets"><span class="header-section-number">1.8</span> Validation Sets and Test Sets</a>
  <ul class="collapse">
  <li><a href="#use-judgment-in-defining-test-sets" id="toc-use-judgment-in-defining-test-sets" class="nav-link" data-scroll-target="#use-judgment-in-defining-test-sets">Use Judgment in Defining Test Sets</a></li>
  </ul></li>
  <li><a href="#a-choose-your-own-adventure-moment" id="toc-a-choose-your-own-adventure-moment" class="nav-link" data-scroll-target="#a-choose-your-own-adventure-moment"><span class="header-section-number">1.9</span> A <em>Choose Your Own Adventure</em> moment</a></li>
  <li><a href="#questionnaire" id="toc-questionnaire" class="nav-link" data-scroll-target="#questionnaire"><span class="header-section-number">1.10</span> Questionnaire</a>
  <ul class="collapse">
  <li><a href="#further-research" id="toc-further-research" class="nav-link" data-scroll-target="#further-research">Further Research</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./intro.html">Getting Started</a></li><li class="breadcrumb-item"><a href="./intro.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Your Deep Learning Journey</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-intro" class="quarto-section-identifier"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Your Deep Learning Journey</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Hello, and thank you for letting us join you on your deep learning journey, however far along that you may be! In this chapter, we will tell you a little bit more about what to expect in this book, introduce the key concepts behind deep learning, and train our first models on different tasks. It doesn’t matter if you don’t come from a technical or a mathematical background (though it’s okay if you do too!); we wrote this book to make deep learning accessible to as many people as possible.</p>
<section id="deep-learning-is-for-everyone" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="deep-learning-is-for-everyone"><span class="header-section-number">1.1</span> Deep Learning Is for Everyone</h2>
<p>A lot of people assume that you need all kinds of hard-to-find stuff to get great results with deep learning, but as you’ll see in this book, those people are wrong. <a href="#tbl-myths" class="quarto-xref">Table&nbsp;<span>1.1</span></a> is a list of a few thing you <em>absolutely don’t need</em> to do world-class deep learning.</p>
<div id="tbl-myths" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-myths-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1.1: What you don’t need to do deep learning
</figcaption>
<div aria-describedby="tbl-myths-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Myth (don’t need)</th>
<th>Truth</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Lots of math</td>
<td>Just high school math is sufficient</td>
</tr>
<tr class="even">
<td>Lots of data</td>
<td>We’ve seen record-breaking results with &lt;50 items of data</td>
</tr>
<tr class="odd">
<td>Lots of expensive computers</td>
<td>You can get what you need for state of the art work for free</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Deep learning is a computer technique to extract and transform data–-with use cases ranging from human speech recognition to animal imagery classification–-by using multiple layers of neural networks. Each of these layers takes its inputs from previous layers and progressively refines them. The layers are trained by algorithms that minimize their errors and improve their accuracy. In this way, the network learns to perform a specified task. We will discuss training algorithms in detail in the next section.</p>
<p>Deep learning has power, flexibility, and simplicity. That’s why we believe it should be applied across many disciplines. These include the social and physical sciences, the arts, medicine, finance, scientific research, and many more. To give a personal example, despite having no background in medicine, Jeremy started Enlitic, a company that uses deep learning algorithms to diagnose illness and disease. Within months of starting the company, it was announced that its algorithm could identify malignant tumors <a href="https://www.nytimes.com/2016/02/29/technology/the-promise-of-artificial-intelligence-unfolds-in-small-steps.html">more accurately than radiologists</a>.</p>
<p>Here’s a list of some of the thousands of tasks in different areas at which deep learning, or methods heavily using deep learning, is now the best in the world:</p>
<ul>
<li>Natural language processing (NLP):: Answering questions; speech recognition; summarizing documents; classifying documents; finding names, dates, etc. in documents; searching for articles mentioning a concept</li>
<li>Computer vision:: Satellite and drone imagery interpretation (e.g., for disaster resilience); face recognition; image captioning; reading traffic signs; locating pedestrians and vehicles in autonomous vehicles</li>
<li>Medicine:: Finding anomalies in radiology images, including CT, MRI, and X-ray images; counting features in pathology slides; measuring features in ultrasounds; diagnosing diabetic retinopathy</li>
<li>Biology:: Folding proteins; classifying proteins; many genomics tasks, such as tumor-normal sequencing and classifying clinically actionable genetic mutations; cell classification; analyzing protein/protein interactions</li>
<li>Image generation:: Colorizing images; increasing image resolution; removing noise from images; converting images to art in the style of famous artists</li>
<li>Recommendation systems:: Web search; product recommendations; home page layout</li>
<li>Playing games:: Chess, Go, most Atari video games, and many real-time strategy games</li>
<li>Robotics:: Handling objects that are challenging to locate (e.g., transparent, shiny, lacking texture) or hard to pick up</li>
<li>Other applications:: Financial and logistical forecasting, text to speech, and much more…</li>
</ul>
<p>What is remarkable is that deep learning has such varied application yet nearly all of deep learning is based on a single type of model, the neural network.</p>
<p>But neural networks are not in fact completely new. In order to have a wider perspective on the field, it is worth it to start with a bit of history.</p>
</section>
<section id="neural-networks-a-brief-history" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="neural-networks-a-brief-history"><span class="header-section-number">1.2</span> Neural Networks: A Brief History</h2>
<p>In 1943 Warren McCulloch, a neurophysiologist, and Walter Pitts, a logician, teamed up to develop a mathematical model of an artificial neuron. In their <a href="https://link.springer.com/article/10.1007/BF02478259">paper</a> “A Logical Calculus of the Ideas Immanent in Nervous Activity” they declared that:</p>
<blockquote class="blockquote">
<p>Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms.</p>
</blockquote>
<p>McCulloch and Pitts realized that a simplified model of a real neuron could be represented using simple addition and thresholding, as shown in <a href="#fig-neuron" class="quarto-xref">Figure&nbsp;<span>1.1</span></a>. Pitts was self-taught, and by age 12, had received an offer to study at Cambridge University with the great Bertrand Russell. He did not take up this invitation, and indeed throughout his life did not accept any offers of advanced degrees or positions of authority. Most of his famous work was done while he was homeless. Despite his lack of an officially recognized position and increasing social isolation, his work with McCulloch was influential, and was taken up by a psychologist named Frank Rosenblatt.</p>
<div id="fig-neuron" class="preview-image quarto-float quarto-figure quarto-figure-center anchored" alt="Natural and artificial neurons">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-neuron-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/chapter7_neuron.png" class="preview-image img-fluid figure-img" alt="Natural and artificial neurons" width="500">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-neuron-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.1: Natural and artificial neurons
</figcaption>
</figure>
</div>
<p>Rosenblatt further developed the artificial neuron to give it the ability to learn. Even more importantly, he worked on building the first device that actually used these principles, the Mark I Perceptron. In “The Design of an Intelligent Automaton” Rosenblatt wrote about this work: “We are now about to witness the birth of such a machine–-a machine capable of perceiving, recognizing and identifying its surroundings without any human training or control.” The perceptron was built, and was able to successfully recognize simple shapes.</p>
<p>An MIT professor named Marvin Minsky (who was a grade behind Rosenblatt at the same high school!), along with Seymour Papert, wrote a book called <em>Perceptrons</em> (MIT Press), about Rosenblatt’s invention. They showed that a single layer of these devices was unable to learn some simple but critical mathematical functions (such as XOR). In the same book, they also showed that using multiple layers of the devices would allow these limitations to be addressed. Unfortunately, only the first of these insights was widely recognized. As a result, the global academic community nearly entirely gave up on neural networks for the next two decades.</p>
<p>Perhaps the most pivotal work in neural networks in the last 50 years was the multi-volume <em>Parallel Distributed Processing</em> (PDP) by David Rumelhart, James McClellan, and the PDP Research Group, released in 1986 by MIT Press. Chapter 1 lays out a similar hope to that shown by Rosenblatt:</p>
<blockquote class="blockquote">
<p>People are smarter than today’s computers because the brain employs a basic computational architecture that is more suited to deal with a central aspect of the natural information processing tasks that people are so good at. …We will introduce a computational framework for modeling cognitive processes that seems… closer than other frameworks to the style of computation as it might be done by the brain.</p>
</blockquote>
<p>The premise that PDP is using here is that traditional computer programs work very differently to brains, and that might be why computer programs had been (at that point) so bad at doing things that brains find easy (such as recognizing objects in pictures). The authors claimed that the PDP approach was “closer than other frameworks” to how the brain works, and therefore it might be better able to handle these kinds of tasks.</p>
<p>In fact, the approach laid out in PDP is very similar to the approach used in today’s neural networks. The book defined parallel distributed processing as requiring:</p>
<ol type="1">
<li>A set of <em>processing units</em></li>
<li>A <em>state of activation</em></li>
<li>An <em>output function</em> for each unit</li>
<li>A <em>pattern of connectivity</em> among units</li>
<li>A <em>propagation rule</em> for propagating patterns of activities through the network of connectivities</li>
<li>An <em>activation rule</em> for combining the inputs impinging on a unit with the current state of that unit to produce an output for the unit</li>
<li>A <em>learning rule</em> whereby patterns of connectivity are modified by experience</li>
<li>An <em>environment</em> within which the system must operate</li>
</ol>
<p>We will see in this book that modern neural networks handle each of these requirements.</p>
<p>In the 1980’s most models were built with a second layer of neurons, thus avoiding the problem that had been identified by Minsky and Papert (this was their “pattern of connectivity among units,” to use the framework above). And indeed, neural networks were widely used during the ’80s and ’90s for real, practical projects. However, again a misunderstanding of the theoretical issues held back the field. In theory, adding just one extra layer of neurons was enough to allow any mathematical function to be approximated with these neural networks, but in practice such networks were often too big and too slow to be useful.</p>
<p>Although researchers showed 30 years ago that to get practical good performance you need to use even more layers of neurons, it is only in the last decade that this principle has been more widely appreciated and applied. Neural networks are now finally living up to their potential, thanks to the use of more layers, coupled with the capacity to do so due to improvements in computer hardware, increases in data availability, and algorithmic tweaks that allow neural networks to be trained faster and more easily. We now have what Rosenblatt promised: “a machine capable of perceiving, recognizing, and identifying its surroundings without any human training or control.”</p>
<p>This is what you will learn how to build in this book. But first, since we are going to be spending a lot of time together, let’s get to know each other a bit…</p>
</section>
<section id="who-we-are" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="who-we-are"><span class="header-section-number">1.3</span> Who We Are</h2>
<p>We are Sylvain and Jeremy, your guides on this journey. We hope that you will find us well suited for this position.</p>
<p>Jeremy has been using and teaching machine learning for around 30 years. He started using neural networks 25 years ago. During this time, he has led many companies and projects that have machine learning at their core, including founding the first company to focus on deep learning and medicine, Enlitic, and taking on the role of President and Chief Scientist of the world’s largest machine learning community, Kaggle. He is the co-founder, along with Dr.&nbsp;Rachel Thomas, of fast.ai, the organization that built the course this book is based on.</p>
<p>From time to time you will hear directly from us, in sidebars like this one from Jeremy:</p>
<blockquote class="blockquote">
<p>J: Hi everybody, I’m Jeremy! You might be interested to know that I do not have any formal technical education. I completed a BA, with a major in philosophy, and didn’t have great grades. I was much more interested in doing real projects, rather than theoretical studies, so I worked full time at a management consulting firm called McKinsey and Company throughout my university years. If you’re somebody who would rather get their hands dirty building stuff than spend years learning abstract concepts, then you will understand where I am coming from! Look out for sidebars from me to find information most suited to people with a less mathematical or formal technical background—that is, people like me…</p>
</blockquote>
<p>Sylvain, on the other hand, knows a lot about formal technical education. In fact, he has written 10 math textbooks, covering the entire advanced French maths curriculum!</p>
<blockquote class="blockquote">
<p>S: Unlike Jeremy, I have not spent many years coding and applying machine learning algorithms. Rather, I recently came to the machine learning world, by watching Jeremy’s fast.ai course videos. So, if you are somebody who has not opened a terminal and written commands at the command line, then you will understand where I am coming from! Look out for sidebars from me to find information most suited to people with a more mathematical or formal technical background, but less real-world coding experience—that is, people like me…</p>
</blockquote>
<p>The fast.ai course has been studied by hundreds of thousands of students, from all walks of life, from all parts of the world. Sylvain stood out as the most impressive student of the course that Jeremy had ever seen, which led to him joining fast.ai, and then becoming the coauthor, along with Jeremy, of the fastai software library.</p>
<p>All this means that between us you have the best of both worlds: the people who know more about the software than anybody else, because they wrote it; an expert on math, and an expert on coding and machine learning; and also people who understand both what it feels like to be a relative outsider in math, and a relative outsider in coding and machine learning.</p>
<p>Anybody who has watched sports knows that if you have a two-person commentary team then you also need a third person to do “special comments.” Our special commentator is Alexis Gallagher. Alexis has a very diverse background: he has been a researcher in mathematical biology, a screenplay writer, an improv performer, a McKinsey consultant (like Jeremy!), a Swift coder, and a CTO.</p>
<blockquote class="blockquote">
<p>A: I’ve decided it’s time for me to learn about this AI stuff! After all, I’ve tried pretty much everything else… But I don’t really have a background in building machine learning models. Still… how hard can it be? I’m going to be learning throughout this book, just like you are. Look out for my sidebars for learning tips that I found helpful on my journey, and hopefully you will find helpful too.</p>
</blockquote>
</section>
<section id="how-to-learn-deep-learning" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="how-to-learn-deep-learning"><span class="header-section-number">1.4</span> How to Learn Deep Learning</h2>
<p>Harvard professor David Perkins, who wrote <em>Making Learning Whole</em> (Jossey-Bass), has much to say about teaching. The basic idea is to teach the <em>whole game</em>. That means that if you’re teaching baseball, you first take people to a baseball game or get them to play it. You don’t teach them how to wind twine to make a baseball from scratch, the physics of a parabola, or the coefficient of friction of a ball on a bat.</p>
<p>Paul Lockhart, a Columbia math PhD, former Brown professor, and K-12 math teacher, imagines in the influential <a href="https://www.maa.org/external_archive/devlin/LockhartsLament.pdf">essay</a> “A Mathematician’s Lament” a nightmare world where music and art are taught the way math is taught. Children are not allowed to listen to or play music until they have spent over a decade mastering music notation and theory, spending classes transposing sheet music into a different key. In art class, students study colors and applicators, but aren’t allowed to actually paint until college. Sound absurd? This is how math is taught–-we require students to spend years doing rote memorization and learning dry, disconnected <em>fundamentals</em> that we claim will pay off later, long after most of them quit the subject.</p>
<p>Unfortunately, this is where many teaching resources on deep learning begin–-asking learners to follow along with the definition of the Hessian and theorems for the Taylor approximation of your loss functions, without ever giving examples of actual working code. We’re not knocking calculus. We love calculus, and Sylvain has even taught it at the college level, but we don’t think it’s the best place to start when learning deep learning!</p>
<p>In deep learning, it really helps if you have the motivation to fix your model to get it to do better. That’s when you start learning the relevant theory. But you need to have the model in the first place. We teach almost everything through real examples. As we build out those examples, we go deeper and deeper, and we’ll show you how to make your projects better and better. This means that you’ll be gradually learning all the theoretical foundations you need, in context, in such a way that you’ll see why it matters and how it works.</p>
<p>So, here’s our commitment to you. Throughout this book, we will follow these principles:</p>
<ul>
<li>Teaching the <em>whole game</em>. We’ll start by showing how to use a complete, working, very usable, state-of-the-art deep learning network to solve real-world problems, using simple, expressive tools. And then we’ll gradually dig deeper and deeper into understanding how those tools are made, and how the tools that make those tools are made, and so on…</li>
<li>Always teaching through examples. We’ll ensure that there is a context and a purpose that you can understand intuitively, rather than starting with algebraic symbol manipulation.</li>
<li>Simplifying as much as possible. We’ve spent years building tools and teaching methods that make previously complex topics very simple.</li>
<li>Removing barriers. Deep learning has, until now, been a very exclusive game. We’re breaking it open, and ensuring that everyone can play.</li>
</ul>
<p>The hardest part of deep learning is artisanal: how do you know if you’ve got enough data, whether it is in the right format, if your model is training properly, and, if it’s not, what you should do about it? That is why we believe in learning by doing. As with basic data science skills, with deep learning you only get better through practical experience. Trying to spend too much time on the theory can be counterproductive. The key is to just code and try to solve problems: the theory can come later, when you have context and motivation.</p>
<p>There will be times when the journey will feel hard. Times where you feel stuck. Don’t give up! Rewind through the book to find the last bit where you definitely weren’t stuck, and then read slowly through from there to find the first thing that isn’t clear. Then try some code experiments yourself, and Google around for more tutorials on whatever the issue you’re stuck with is—often you’ll find some different angle on the material might help it to click. Also, it’s expected and normal to not understand everything (especially the code) on first reading. Trying to understand the material serially before proceeding can sometimes be hard. Sometimes things click into place after you get more context from parts down the road, from having a bigger picture. So if you do get stuck on a section, try moving on anyway and make a note to come back to it later.</p>
<p>Remember, you don’t need any particular academic background to succeed at deep learning. Many important breakthroughs are made in research and industry by folks without a PhD, such as <a href="https://arxiv.org/abs/1511.06434">“Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks”</a>—one of the most influential papers of the last decade—with over 5,000 citations, which was written by Alec Radford when he was an undergraduate. Even at Tesla, where they’re trying to solve the extremely tough challenge of making a self-driving car, CEO <a href="https://twitter.com/elonmusk/status/1224089444963311616">Elon Musk says</a>:</p>
<blockquote class="blockquote">
<p>A PhD is definitely not required. All that matters is a deep understanding of AI &amp; ability to implement NNs in a way that is actually useful (latter point is what’s truly hard). Don’t care if you even graduated high school.</p>
</blockquote>
<p>What you will need to do to succeed however is to apply what you learn in this book to a personal project, and always persevere.</p>
<section id="your-projects-and-your-mindset" class="level3">
<h3 class="anchored" data-anchor-id="your-projects-and-your-mindset">Your Projects and Your Mindset</h3>
<p>Whether you’re excited to identify if plants are diseased from pictures of their leaves, auto-generate knitting patterns, diagnose TB from X-rays, or determine when a raccoon is using your cat door, we will get you using deep learning on your own problems (via pre-trained models from others) as quickly as possible, and then will progressively drill into more details. You’ll learn how to use deep learning to solve your own problems at state-of-the-art accuracy within the first 30 minutes of the next chapter! (And feel free to skip straight there now if you’re dying to get coding right away.) There is a pernicious myth out there that you need to have computing resources and datasets the size of those at Google to be able to do deep learning, but it’s not true.</p>
<p>So, what sorts of tasks make for good test cases? You could train your model to distinguish between Picasso and Monet paintings or to pick out pictures of your daughter instead of pictures of your son. It helps to focus on your hobbies and passions–-setting yourself four or five little projects rather than striving to solve a big, grand problem tends to work better when you’re getting started. Since it is easy to get stuck, trying to be too ambitious too early can often backfire. Then, once you’ve got the basics mastered, aim to complete something you’re really proud of!</p>
<blockquote class="blockquote">
<p>J: Deep learning can be set to work on almost any problem. For instance, my first startup was a company called FastMail, which provided enhanced email services when it launched in 1999 (and still does to this day). In 2002 I set it up to use a primitive form of deep learning, single-layer neural networks, to help categorize emails and stop customers from receiving spam.</p>
</blockquote>
<p>Common character traits in the people that do well at deep learning include playfulness and curiosity. The late physicist Richard Feynman is an example of someone who we’d expect to be great at deep learning: his development of an understanding of the movement of subatomic particles came from his amusement at how plates wobble when they spin in the air.</p>
<p>Let’s now focus on what you will learn, starting with the software.</p>
</section>
</section>
<section id="the-software-pytorch-fastai-and-jupyter" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="the-software-pytorch-fastai-and-jupyter"><span class="header-section-number">1.5</span> The Software: PyTorch, fastai, and Jupyter</h2>
<p>(And Why It Doesn’t Matter)</p>
<p>We’ve completed hundreds of machine learning projects using dozens of different packages, and many different programming languages. At fast.ai, we have written courses using most of the main deep learning and machine learning packages used today. After PyTorch came out in 2017 we spent over a thousand hours testing it before deciding that we would use it for future courses, software development, and research. Since that time PyTorch has become the world’s fastest-growing deep learning library and is already used for most research papers at top conferences. This is generally a leading indicator of usage in industry, because these are the papers that end up getting used in products and services commercially. We have found that PyTorch is the most flexible and expressive library for deep learning. It does not trade off speed for simplicity, but provides both.</p>
<p>PyTorch works best as a low-level foundation library, providing the basic operations for higher-level functionality. The fastai library is the most popular library for adding this higher-level functionality on top of PyTorch. It’s also particularly well suited to the purposes of this book, because it is unique in providing a deeply layered software architecture (there’s even a <a href="https://arxiv.org/abs/2002.04688">peer-reviewed academic paper</a> about this layered API). In this book, as we go deeper and deeper into the foundations of deep learning, we will also go deeper and deeper into the layers of fastai. This book covers version 2 of the fastai library, which is a from-scratch rewrite providing many unique features.</p>
<p>However, it doesn’t really matter what software you learn, because it takes only a few days to learn to switch from one library to another. What really matters is learning the deep learning foundations and techniques properly. Our focus will be on using code that clearly expresses the concepts that you need to learn. Where we are teaching high-level concepts, we will use high-level fastai code. Where we are teaching low-level concepts, we will use low-level PyTorch, or even pure Python code.</p>
<p>If it feels like new deep learning libraries are appearing at a rapid pace nowadays, then you need to be prepared for a much faster rate of change in the coming months and years. As more people enter the field, they will bring more skills and ideas, and try more things. You should assume that whatever specific libraries and software you learn today will be obsolete in a year or two. Just think about the number of changes in libraries and technology stacks that occur all the time in the world of web programming—a much more mature and slow-growing area than deep learning. We strongly believe that the focus in learning needs to be on understanding the underlying techniques and how to apply them in practice, and how to quickly build expertise in new tools and techniques as they are released.</p>
<p>By the end of the book, you’ll understand nearly all the code that’s inside fastai (and much of PyTorch too), because in each chapter we’ll be digging a level deeper to show you exactly what’s going on as we build and train our models. This means that you’ll have learned the most important best practices used in modern deep learning—not just how to use them, but how they really work and are implemented. If you want to use those approaches in another framework, you’ll have the knowledge you need to do so if needed.</p>
<p>Since the most important thing for learning deep learning is writing code and experimenting, it’s important that you have a great platform for experimenting with code. The most popular programming experimentation platform is called Jupyter. This is what we will be using throughout this book. We will show you how you can use Jupyter to train and experiment with models and introspect every stage of the data pre-processing and model development pipeline. <a href="https://jupyter.org/">Jupyter Notebook</a> is the most popular tool for doing data science in Python, for good reason. It is powerful, flexible, and easy to use. We think you will love it!</p>
<p>Let’s see it in practice and train our first model.</p>
</section>
<section id="your-first-model" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="your-first-model"><span class="header-section-number">1.6</span> Your First Model</h2>
<p>As we said before, we will teach you how to do things before we explain why they work. Following this top-down approach, we will begin by actually training an image classifier to recognize dogs and cats with almost 100% accuracy. To train this model and run our experiments, you will need to do some initial setup. Don’t worry, it’s not as hard as it looks.</p>
<blockquote class="blockquote">
<p>s: Do not skip the setup part even if it looks intimidating at first, especially if you have little or no experience using things like a terminal or the command line. Most of that is actually not necessary and you will find that the easiest servers can be set up with just your usual web browser. It is crucial that you run your own experiments in parallel with this book in order to learn.</p>
</blockquote>
<section id="getting-a-gpu-deep-learning-server" class="level3">
<h3 class="anchored" data-anchor-id="getting-a-gpu-deep-learning-server">Getting a GPU Deep Learning Server</h3>
<p>To do nearly everything in this book, you’ll need access to a computer with an NVIDIA GPU (unfortunately other brands of GPU are not fully supported by the main deep learning libraries). However, we don’t recommend you buy one; in fact, even if you already have one, we don’t suggest you use it just yet! Setting up a computer takes time and energy, and you want all your energy to focus on deep learning right now. Therefore, we instead suggest you rent access to a computer that already has everything you need preinstalled and ready to go. Costs can be as little as US$0.25 per hour while you’re using it, and some options are even free.</p>
<blockquote class="blockquote">
<p>jargon: Graphics Processing Unit (GPU): Also known as a <em>graphics card</em>. A special kind of processor in your computer that can handle thousands of single tasks at the same time, especially designed for displaying 3D environments on a computer for playing games. These same basic tasks are very similar to what neural networks do, such that GPUs can run neural networks hundreds of times faster than regular CPUs. All modern computers contain a GPU, but few contain the right kind of GPU necessary for deep learning.</p>
</blockquote>
<p>The best choice of GPU servers to use with this book will change over time, as companies come and go and prices change. We maintain a list of our recommended options on the <a href="https://book.fast.ai/">book’s website</a>, so go there now and follow the instructions to get connected to a GPU deep learning server. Don’t worry, it only takes about two minutes to get set up on most platforms, and many don’t even require any payment, or even a credit card, to get started.</p>
<blockquote class="blockquote">
<p>A: My two cents: heed this advice! If you like computers you will be tempted to set up your own box. Beware! It is feasible but surprisingly involved and distracting. There is a good reason this book is not titled, <em>Everything You Ever Wanted to Know About Ubuntu System Administration, NVIDIA Driver Installation, apt-get, conda, pip, and Jupyter Notebook Configuration</em>. That would be a book of its own. Having designed and deployed our production machine learning infrastructure at work, I can testify it has its satisfactions, but it is as unrelated to modeling as maintaining an airplane is to flying one.</p>
</blockquote>
<p>Each option shown on the website includes a tutorial; after completing the tutorial, you will end up with a screen looking like <a href="#fig-notebook-init" class="quarto-xref">Figure&nbsp;<span>1.2</span></a>.</p>
<div id="fig-notebook-init" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Initial view of Jupyter Notebook">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-notebook-init-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/att_00057.png" class="img-fluid figure-img" alt="Initial view of Jupyter Notebook" width="658">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-notebook-init-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.2: Initial view of Jupyter Notebook
</figcaption>
</figure>
</div>
<p>You are now ready to run your first Jupyter notebook!</p>
<blockquote class="blockquote">
<p>jargon: Jupyter Notebook: A piece of software that allows you to include formatted text, code, images, videos, and much more, all within a single interactive document. Jupyter received the highest honor for software, the ACM Software System Award, thanks to its wide use and enormous impact in many academic fields and in industry. Jupyter Notebook is the software most widely used by data scientists for developing and interacting with deep learning models.</p>
</blockquote>
</section>
<section id="running-your-first-notebook" class="level3">
<h3 class="anchored" data-anchor-id="running-your-first-notebook">Running Your First Notebook</h3>
<p>The notebooks are labeled by chapter and then by notebook number, so that they are in the same order as they are presented in this book. So, the very first notebook you will see listed is the notebook that you need to use now. You will be using this notebook to train a model that can recognize dog and cat photos. To do this, you’ll be downloading a <em>dataset</em> of dog and cat photos, and using that to <em>train a model</em>. A dataset is simply a bunch of data—it could be images, emails, financial indicators, sounds, or anything else. There are many datasets made freely available that are suitable for training models. Many of these datasets are created by academics to help advance research, many are made available for competitions (there are competitions where data scientists can compete to see who has the most accurate model!), and some are by-products of other processes (such as financial filings).</p>
<blockquote class="blockquote">
<p>note: Full and Stripped Notebooks: There are two folders containing different versions of the notebooks. The <em>full</em> folder contains the exact notebooks used to create the book you’re reading now, with all the prose and outputs. The <em>stripped</em> version has the same headings and code cells, but all outputs and prose have been removed. After reading a section of the book, we recommend working through the stripped notebooks, with the book closed, and seeing if you can figure out what each cell will show before you execute it. Also try to recall what the code is demonstrating.</p>
</blockquote>
<p>To open a notebook, just click on it. The notebook will open, and it will look something like <a href="#fig-jupyter" class="quarto-xref">Figure&nbsp;<span>1.3</span></a> (note that there may be slight differences in details across different platforms; you can ignore those differences).</p>
<div id="fig-jupyter" class="quarto-float quarto-figure quarto-figure-center anchored" alt="An example of notebook">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-jupyter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/0_jupyter.png" class="img-fluid figure-img" alt="An example of notebook" width="700">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-jupyter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.3: A Jupyter notebook
</figcaption>
</figure>
</div>
<p>A notebook consists of <em>cells</em>. There are two main types of cell:</p>
<ul>
<li>Cells containing formatted text, images, and so forth. These use a format called <em>markdown</em>, which you will learn about soon.</li>
<li>Cells containing code that can be executed, and outputs will appear immediately underneath (which could be plain text, tables, images, animations, sounds, or even interactive applications).</li>
</ul>
<p>Jupyter notebooks can be in one of two modes: edit mode or command mode. In edit mode typing on your keyboard enters the letters into the cell in the usual way. However, in command mode, you will not see any flashing cursor, and the keys on your keyboard will each have a special function.</p>
<p>Before continuing, press the Escape key on your keyboard to switch to command mode (if you are already in command mode, this does nothing, so press it now just in case). To see a complete list of all of the functions available, press H; press Escape to remove this help screen. Notice that in command mode, unlike most programs, commands do not require you to hold down Control, Alt, or similar—you simply press the required letter key.</p>
<p>You can make a copy of a cell by pressing C (the cell needs to be selected first, indicated with an outline around it; if it is not already selected, click on it once). Then press V to paste a copy of it.</p>
<p>Click on the cell that begins with the line “# CLICK ME” to select it. The first character in that line indicates that what follows is a comment in Python, so it is ignored when executing the cell. The rest of the cell is, believe it or not, a complete system for creating and training a state-of-the-art model for recognizing cats versus dogs. So, let’s train it now! To do so, just press Shift-Enter on your keyboard, or press the Play button on the toolbar. Then wait a few minutes while the following things happen:</p>
<ol type="1">
<li>A dataset called the <a href="http://www.robots.ox.ac.uk/~vgg/data/pets/">Oxford-IIIT Pet Dataset</a> that contains 7,349 images of cats and dogs from 37 different breeds will be downloaded from the fast.ai datasets collection to the GPU server you are using, and will then be extracted.</li>
<li>A <em>pretrained model</em> that has already been trained on 1.3 million images, using a competition-winning model will be downloaded from the internet.</li>
<li>The pretrained model will be <em>fine-tuned</em> using the latest advances in transfer learning, to create a model that is specially customized for recognizing dogs and cats.</li>
</ol>
<p>The first two steps only need to be run once on your GPU server. If you run the cell again, it will use the dataset and model that have already been downloaded, rather than downloading them again. Let’s take a look at the contents of the cell, and the results:</p>
<div id="cell-53" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># CLICK ME</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastai.vision.<span class="bu">all</span> <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> untar_data(URLs.PETS)<span class="op">/</span><span class="st">'images'</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> is_cat(x): <span class="cf">return</span> x[<span class="dv">0</span>].isupper()</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> ImageDataLoaders.from_name_func(</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    path, get_image_files(path), valid_pct<span class="op">=</span><span class="fl">0.2</span>, seed<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    label_func<span class="op">=</span>is_cat, item_tfms<span class="op">=</span>Resize(<span class="dv">224</span>))</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> vision_learner(dls, resnet34, metrics<span class="op">=</span>error_rate)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>learn.fine_tune(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.180385</td>
<td>0.023942</td>
<td>0.006766</td>
<td>00:16</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">error_rate</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.056023</td>
<td>0.007580</td>
<td>0.004060</td>
<td>00:20</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>You will probably not see exactly the same results that are in the book. There are a lot of sources of small random variation involved in training models. We generally see an error rate of well less than 0.02 in this example, however.</p>
<blockquote class="blockquote">
<p>important: Training Time: Depending on your network speed, it might take a few minutes to download the pretrained model and dataset. Running <code>fine_tune</code> might take a minute or so. Often models in this book take a few minutes to train, as will your own models, so it’s a good idea to come up with good techniques to make the most of this time. For instance, keep reading the next section while your model trains, or open up another notebook and use it for some coding experiments.</p>
</blockquote>
</section>
<section id="sidebar-this-book-was-written-in-jupyter-notebooks" class="level3">
<h3 class="anchored" data-anchor-id="sidebar-this-book-was-written-in-jupyter-notebooks">Sidebar: This Book Was Written in Jupyter Notebooks</h3>
<p>We wrote this book using Jupyter notebooks, so for nearly every chart, table, and calculation in this book, we’ll be showing you the exact code required to replicate it yourself. That’s why very often in this book, you will see some code immediately followed by a table, a picture or just some text. If you go on the <a href="https://book.fast.ai">book’s website</a> you will find all the code, and you can try running and modifying every example yourself.</p>
<p>You just saw how a cell that outputs a table looks inside the book. Here is an example of a cell that outputs text:</p>
<div id="cell-59" class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="op">+</span><span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>2</code></pre>
</div>
</div>
<p>Jupyter will always print or show the result of the last line (if there is one). For instance, here is an example of a cell that outputs an image:</p>
<div id="cell-61" class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> PILImage.create(image_cat())</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>img.to_thumb(<span class="dv">192</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="intro_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="end-sidebar" class="level3">
<h3 class="anchored" data-anchor-id="end-sidebar">End sidebar</h3>
<p>So, how do we know if this model is any good? In the last column of the table you can see the error rate, which is the proportion of images that were incorrectly identified. The error rate serves as our metric—our measure of model quality, chosen to be intuitive and comprehensible. As you can see, the model is nearly perfect, even though the training time was only a few seconds (not including the one-time downloading of the dataset and the pretrained model). In fact, the accuracy you’ve achieved already is far better than anybody had ever achieved just 10 years ago!</p>
<p>Finally, let’s check that this model actually works. Go and get a photo of a dog, or a cat; if you don’t have one handy, just search Google Images and download an image that you find there. Now execute the cell with <code>uploader</code> defined. It will output a button you can click, so you can select the image you want to classify:</p>
<div id="cell-64" class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>uploader <span class="op">=</span> widgets.FileUpload()</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>uploader</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="fig-upload" class="quarto-float quarto-figure quarto-figure-center anchored" alt="An upload button" width="159">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-upload-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/att_00008.png" id="fig-upload" class="img-fluid figure-img" alt="An upload button" width="159">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-upload-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
</figcaption>
</figure>
</div>
<p>Now you can pass the uploaded file to the model. Make sure that it is a clear photo of a single dog or a cat, and not a line drawing, cartoon, or similar. The notebook will tell you whether it thinks it is a dog or a cat, and how confident it is. Hopefully, you’ll find that your model did a great job:</p>
<div id="cell-68" class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> PILImage.create(uploader.data[<span class="dv">0</span>])</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>is_cat,_,probs <span class="op">=</span> learn.predict(img)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Is this a cat?: </span><span class="sc">{</span>is_cat<span class="sc">}</span><span class="ss">."</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Probability it's a cat: </span><span class="sc">{</span>probs[<span class="dv">1</span>]<span class="sc">.</span>item()<span class="sc">:.6f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-stdout">
<pre><code>Is this a cat?: True.
Probability it's a cat: 1.000000</code></pre>
</div>
</div>
<p>Congratulations on your first classifier!</p>
<p>But what does this mean? What did you actually do? In order to explain this, let’s zoom out again to take in the big picture.</p>
</section>
<section id="what-is-machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="what-is-machine-learning">What Is Machine Learning?</h3>
<p>Your classifier is a deep learning model. As was already mentioned, deep learning models use neural networks, which originally date from the 1950s and have become powerful very recently thanks to recent advancements.</p>
<p>Another key piece of context is that deep learning is just a modern area in the more general discipline of <em>machine learning</em>. To understand the essence of what you did when you trained your own classification model, you don’t need to understand deep learning. It is enough to see how your model and your training process are examples of the concepts that apply to machine learning in general.</p>
<p>So in this section, we will describe what machine learning is. We will look at the key concepts, and show how they can be traced back to the original essay that introduced them.</p>
<p><em>Machine learning</em> is, like regular programming, a way to get computers to complete a specific task. But how would we use regular programming to do what we just did in the last section: recognize dogs versus cats in photos? We would have to write down for the computer the exact steps necessary to complete the task.</p>
<p>Normally, it’s easy enough for us to write down the steps to complete a task when we’re writing a program. We just think about the steps we’d take if we had to do the task by hand, and then we translate them into code. For instance, we can write a function that sorts a list. In general, we’d write a function that looks something like <a href="#fig-basic-program" class="quarto-xref">Figure&nbsp;<span>1.5</span></a> (where <em>inputs</em> might be an unsorted list, and <em>results</em> a sorted list).</p>
<div id="cell-fig-basic-program" class="cell" data-hide_input="false">
<div class="cell-output cell-output-display">
<div id="fig-basic-program" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Pipeline inputs, program, results">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-basic-program-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="intro_files/figure-html/fig-basic-program-output-1.svg" class="img-fluid figure-img" alt="Pipeline inputs, program, results">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-basic-program-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.5: A traditional program
</figcaption>
</figure>
</div>
</div>
</div>
<p>But for recognizing objects in a photo that’s a bit tricky; what <em>are</em> the steps we take when we recognize an object in a picture? We really don’t know, since it all happens in our brain without us being consciously aware of it!</p>
<p>Right back at the dawn of computing, in 1949, an IBM researcher named Arthur Samuel started working on a different way to get computers to complete tasks, which he called <em>machine learning</em>. In his classic 1962 essay “Artificial Intelligence: A Frontier of Automation”, he wrote:</p>
<blockquote class="blockquote">
<p>Programming a computer for such computations is, at best, a difficult task, not primarily because of any inherent complexity in the computer itself but, rather, because of the need to spell out every minute step of the process in the most exasperating detail. Computers, as any programmer will tell you, are giant morons, not giant brains.</p>
</blockquote>
<p>His basic idea was this: instead of telling the computer the exact steps required to solve a problem, show it examples of the problem to solve, and let it figure out how to solve it itself. This turned out to be very effective: by 1961 his checkers-playing program had learned so much that it beat the Connecticut state champion! Here’s how he described his idea (from the same essay as above):</p>
<blockquote class="blockquote">
<p>Suppose we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would “learn” from its experience.</p>
</blockquote>
<p>There are a number of powerful concepts embedded in this short statement:</p>
<ul>
<li>The idea of a “weight assignment”</li>
<li>The fact that every weight assignment has some “actual performance”</li>
<li>The requirement that there be an “automatic means” of testing that performance,<br>
</li>
<li>The need for a “mechanism” (i.e., another automatic process) for improving the performance by changing the weight assignments</li>
</ul>
<p>Let us take these concepts one by one, in order to understand how they fit together in practice. First, we need to understand what Samuel means by a <em>weight assignment</em>.</p>
<p>Weights are just variables, and a weight assignment is a particular choice of values for those variables. The program’s inputs are values that it processes in order to produce its results—for instance, taking image pixels as inputs, and returning the classification “dog” as a result. The program’s weight assignments are other values that define how the program will operate.</p>
<p>Since they will affect the program they are in a sense another kind of input, so we will update our basic picture in <a href="#fig-basic-program" class="quarto-xref">Figure&nbsp;<span>1.5</span></a> and replace it with <a href="#fig-weight-assignment" class="quarto-xref">Figure&nbsp;<span>1.6</span></a> in order to take this into account.</p>
<div id="cell-fig-weight-assignment" class="cell" data-hide_input="true">
<div class="cell-output cell-output-display">
<div id="fig-weight-assignment" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-weight-assignment-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="intro_files/figure-html/fig-weight-assignment-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-weight-assignment-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.6: A program using weight assignment
</figcaption>
</figure>
</div>
</div>
</div>
<p>We’ve changed the name of our box from <em>program</em> to <em>model</em>. This is to follow modern terminology and to reflect that the <em>model</em> is a special kind of program: it’s one that can do <em>many different things</em>, depending on the <em>weights</em>. It can be implemented in many different ways. For instance, in Samuel’s checkers program, different values of the weights would result in different checkers-playing strategies.</p>
<p>(By the way, what Samuel called “weights” are most generally referred to as model <em>parameters</em> these days, in case you have encountered that term. The term <em>weights</em> is reserved for a particular type of model parameter.)</p>
<p>Next, Samuel said we need an <em>automatic means of testing the effectiveness of any current weight assignment in terms of actual performance</em>. In the case of his checkers program, the “actual performance” of a model would be how well it plays. And you could automatically test the performance of two models by setting them to play against each other, and seeing which one usually wins.</p>
<p>Finally, he says we need <em>a mechanism for altering the weight assignment so as to maximize the performance</em>. For instance, we could look at the difference in weights between the winning model and the losing model, and adjust the weights a little further in the winning direction.</p>
<p>We can now see why he said that such a procedure <em>could be made entirely automatic and… a machine so programmed would “learn” from its experience</em>. Learning would become entirely automatic when the adjustment of the weights was also automatic—when instead of us improving a model by adjusting its weights manually, we relied on an automated mechanism that produced adjustments based on performance.</p>
<p><a href="#fig-training-loop" class="quarto-xref">Figure&nbsp;<span>1.7</span></a> shows the full picture of Samuel’s idea of training a machine learning model.</p>
<div id="cell-fig-training-loop" class="cell" data-hide_input="true">
<div class="cell-output cell-output-display">
<div id="fig-training-loop" class="quarto-float quarto-figure quarto-figure-center anchored" alt="The basic training loop">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-training-loop-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="intro_files/figure-html/fig-training-loop-output-1.svg" class="img-fluid figure-img" alt="The basic training loop">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-training-loop-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.7: Training a machine learning model
</figcaption>
</figure>
</div>
</div>
</div>
<p>Notice the distinction between the model’s <em>results</em> (e.g., the moves in a checkers game) and its <em>performance</em> (e.g., whether it wins the game, or how quickly it wins).</p>
<p>Also note that once the model is trained—that is, once we’ve chosen our final, best, favorite weight assignment—then we can think of the weights as being <em>part of the model</em>, since we’re not varying them any more.</p>
<p>Therefore, actually <em>using</em> a model after it’s trained looks like <a href="#fig-using-model" class="quarto-xref">Figure&nbsp;<span>1.8</span></a>.</p>
<div id="cell-fig-using-model" class="cell" data-hide_input="true">
<div class="cell-output cell-output-display">
<div id="fig-using-model" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-using-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="intro_files/figure-html/fig-using-model-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-using-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.8: Using a trained model as a program
</figcaption>
</figure>
</div>
</div>
</div>
<p>This looks identical to our original diagram in <a href="#fig-basic-program" class="quarto-xref">Figure&nbsp;<span>1.5</span></a>, just with the word <em>program</em> replaced with <em>model</em>. This is an important insight: <em>a trained model can be treated just like a regular computer program</em>.</p>
<blockquote class="blockquote">
<p>jargon: Machine Learning: The training of programs developed by allowing a computer to learn from its experience, rather than through manually coding the individual steps.</p>
</blockquote>
</section>
<section id="what-is-a-neural-network" class="level3">
<h3 class="anchored" data-anchor-id="what-is-a-neural-network">What Is a Neural Network?</h3>
<p>It’s not too hard to imagine what the model might look like for a checkers program. There might be a range of checkers strategies encoded, and some kind of search mechanism, and then the weights could vary how strategies are selected, what parts of the board are focused on during a search, and so forth. But it’s not at all obvious what the model might look like for an image recognition program, or for understanding text, or for many other interesting problems we might imagine.</p>
<p>What we would like is some kind of function that is so flexible that it could be used to solve any given problem, just by varying its weights. Amazingly enough, this function actually exists! It’s the neural network, which we already discussed. That is, if you regard a neural network as a mathematical function, it turns out to be a function which is extremely flexible depending on its weights. A mathematical proof called the <em>universal approximation theorem</em> shows that this function can solve any problem to any level of accuracy, in theory. The fact that neural networks are so flexible means that, in practice, they are often a suitable kind of model, and you can focus your effort on the process of training them—that is, of finding good weight assignments.</p>
<p>But what about that process? One could imagine that you might need to find a new “mechanism” for automatically updating weights for every problem. This would be laborious. What we’d like here as well is a completely general way to update the weights of a neural network, to make it improve at any given task. Conveniently, this also exists!</p>
<p>This is called <em>stochastic gradient descent</em> (SGD). We’ll see how neural networks and SGD work in detail in <a href="mnist_basics.html" class="quarto-xref"><span>Chapter 4</span></a>, as well as explaining the universal approximation theorem. For now, however, we will instead use Samuel’s own words: <em>We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would “learn” from its experience.</em></p>
<blockquote class="blockquote">
<p>J: Don’t worry, neither SGD nor neural nets are mathematically complex. Both nearly entirely rely on addition and multiplication to do their work (but they do a <em>lot</em> of addition and multiplication!). The main reaction we hear from students when they see the details is: “Is that all it is?”</p>
</blockquote>
<p>In other words, to recap, a neural network is a particular kind of machine learning model, which fits right in to Samuel’s original conception. Neural networks are special because they are highly flexible, which means they can solve an unusually wide range of problems just by finding the right weights. This is powerful, because stochastic gradient descent provides us a way to find those weight values automatically.</p>
<p>Having zoomed out, let’s now zoom back in and revisit our image classification problem using Samuel’s framework.</p>
<p>Our inputs are the images. Our weights are the weights in the neural net. Our model is a neural net. Our results are the values that are calculated by the neural net, like “dog” or “cat.”</p>
<p>What about the next piece, an <em>automatic means of testing the effectiveness of any current weight assignment in terms of actual performance</em>? Determining “actual performance” is easy enough: we can simply define our model’s performance as its accuracy at predicting the correct answers.</p>
<p>Putting this all together, and assuming that SGD is our mechanism for updating the weight assignments, we can see how our image classifier is a machine learning model, much like Samuel envisioned.</p>
</section>
<section id="a-bit-of-deep-learning-jargon" class="level3">
<h3 class="anchored" data-anchor-id="a-bit-of-deep-learning-jargon">A Bit of Deep Learning Jargon</h3>
<p>Samuel was working in the 1960s, and since then terminology has changed. Here is the modern deep learning terminology for all the pieces we have discussed:</p>
<ul>
<li>The functional form of the <em>model</em> is called its <em>architecture</em> (but be careful—sometimes people use <em>model</em> as a synonym of <em>architecture</em>, so this can get confusing).</li>
<li>The <em>weights</em> are called <em>parameters</em>.</li>
<li>The <em>predictions</em> are calculated from the <em>independent variable</em>, which is the <em>data</em> not including the <em>labels</em>.</li>
<li>The <em>results</em> of the model are called <em>predictions</em>.</li>
<li>The measure of <em>performance</em> is called the <em>loss</em>.</li>
<li>The loss depends not only on the predictions, but also the correct <em>labels</em> (also known as <em>targets</em> or the <em>dependent variable</em>); e.g., “dog” or “cat.”</li>
</ul>
<p>After making these changes, our diagram in <a href="#fig-training-loop" class="quarto-xref">Figure&nbsp;<span>1.7</span></a> looks like <a href="#fig-detailed-loop" class="quarto-xref">Figure&nbsp;<span>1.9</span></a>.</p>
<div id="cell-fig-detailed-loop" class="cell" data-hide_input="true">
<div class="cell-output cell-output-display">
<div id="fig-detailed-loop" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-detailed-loop-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="intro_files/figure-html/fig-detailed-loop-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-detailed-loop-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.9: Detailed training loop
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="limitations-inherent-to-machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="limitations-inherent-to-machine-learning">Limitations Inherent To Machine Learning</h3>
<p>From this picture we can now see some fundamental things about training a deep learning model:</p>
<ul>
<li>A model cannot be created without data.</li>
<li>A model can only learn to operate on the patterns seen in the input data used to train it.</li>
<li>This learning approach only creates <em>predictions</em>, not recommended <em>actions</em>.</li>
<li>It’s not enough to just have examples of input data; we need <em>labels</em> for that data too (e.g., pictures of dogs and cats aren’t enough to train a model; we need a label for each one, saying which ones are dogs, and which are cats).</li>
</ul>
<p>Generally speaking, we’ve seen that most organizations that say they don’t have enough data, actually mean they don’t have enough <em>labeled</em> data. If any organization is interested in doing something in practice with a model, then presumably they have some inputs they plan to run their model against. And presumably they’ve been doing that some other way for a while (e.g., manually, or with some heuristic program), so they have data from those processes! For instance, a radiology practice will almost certainly have an archive of medical scans (since they need to be able to check how their patients are progressing over time), but those scans may not have structured labels containing a list of diagnoses or interventions (since radiologists generally create free-text natural language reports, not structured data). We’ll be discussing labeling approaches a lot in this book, because it’s such an important issue in practice.</p>
<p>Since these kinds of machine learning models can only make <em>predictions</em> (i.e., attempt to replicate labels), this can result in a significant gap between organizational goals and model capabilities. For instance, in this book you’ll learn how to create a <em>recommendation system</em> that can predict what products a user might purchase. This is often used in e-commerce, such as to customize products shown on a home page by showing the highest-ranked items. But such a model is generally created by looking at a user and their buying history (<em>inputs</em>) and what they went on to buy or look at (<em>labels</em>), which means that the model is likely to tell you about products the user already has or already knows about, rather than new products that they are most likely to be interested in hearing about. That’s very different to what, say, an expert at your local bookseller might do, where they ask questions to figure out your taste, and then tell you about authors or series that you’ve never heard of before.</p>
<p>Another critical insight comes from considering how a model interacts with its environment. This can create <em>feedback loops</em>, as described here:</p>
<ul>
<li>A <em>predictive policing</em> model is created based on where arrests have been made in the past. In practice, this is not actually predicting crime, but rather predicting arrests, and is therefore partially simply reflecting biases in existing policing processes.</li>
<li>Law enforcement officers then might use that model to decide where to focus their police activity, resulting in increased arrests in those areas.</li>
<li>Data on these additional arrests would then be fed back in to retrain future versions of the model.</li>
</ul>
<p>This is a <em>positive feedback loop</em>, where the more the model is used, the more biased the data becomes, making the model even more biased, and so forth.</p>
<p>Feedback loops can also create problems in commercial settings. For instance, a video recommendation system might be biased toward recommending content consumed by the biggest watchers of video (e.g., conspiracy theorists and extremists tend to watch more online video content than the average), resulting in those users increasing their video consumption, resulting in more of those kinds of videos being recommended. We’ll consider this topic more in detail in <a href="book3.html" class="quarto-xref"><span>Chapter 3</span></a>.</p>
<p>Now that you have seen the base of the theory, let’s go back to our code example and see in detail how the code corresponds to the process we just described.</p>
</section>
<section id="how-our-image-recognizer-works" class="level3">
<h3 class="anchored" data-anchor-id="how-our-image-recognizer-works">How Our Image Recognizer Works</h3>
<p>Let’s see just how our image recognizer code maps to these ideas. We’ll put each line into a separate cell, and look at what each one is doing (we won’t explain every detail of every parameter yet, but will give a description of the important bits; full details will come later in the book).</p>
<p>The first line imports all of the fastai.vision library.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastai.vision.<span class="bu">all</span> <span class="im">import</span> <span class="op">*</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This gives us all of the functions and classes we will need to create a wide variety of computer vision models.</p>
<blockquote class="blockquote">
<p>J: A lot of Python coders recommend avoiding importing a whole library like this (using the <code>import *</code> syntax), because in large software projects it can cause problems. However, for interactive work such as in a Jupyter notebook, it works great. The fastai library is specially designed to support this kind of interactive use, and it will only import the necessary pieces into your environment.</p>
</blockquote>
<p>The second line downloads a standard dataset from the <a href="https://course.fast.ai/datasets">fast.ai datasets collection</a> (if not previously downloaded) to your server, extracts it (if not previously extracted), and returns a <code>Path</code> object with the extracted location:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> untar_data(URLs.PETS)<span class="op">/</span><span class="st">'images'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>S: Throughout my time studying at fast.ai, and even still today, I’ve learned a lot about productive coding practices. The fastai library and fast.ai notebooks are full of great little tips that have helped make me a better programmer. For instance, notice that the fastai library doesn’t just return a string containing the path to the dataset, but a <code>Path</code> object. This is a really useful class from the Python 3 standard library that makes accessing files and directories much easier. If you haven’t come across it before, be sure to check out its documentation or a tutorial and try it out. Note that the https://book.fast.ai[website] contains links to recommended tutorials for each chapter. I’ll keep letting you know about little coding tips I’ve found useful as we come across them.</p>
</blockquote>
<p>In the third line we define a function, <code>is_cat</code>, which labels cats based on a filename rule provided by the dataset creators:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> is_cat(x): <span class="cf">return</span> x[<span class="dv">0</span>].isupper()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We use that function in the fourth line, which tells fastai what kind of dataset we have and how it is structured:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> ImageDataLoaders.from_name_func(</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    path, get_image_files(path), valid_pct<span class="op">=</span><span class="fl">0.2</span>, seed<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    label_func<span class="op">=</span>is_cat, item_tfms<span class="op">=</span>Resize(<span class="dv">224</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>There are various different classes for different kinds of deep learning datasets and problems—here we’re using <code>ImageDataLoaders</code>. The first part of the class name will generally be the type of data you have, such as image, or text.</p>
<p>The other important piece of information that we have to tell fastai is how to get the labels from the dataset. Computer vision datasets are normally structured in such a way that the label for an image is part of the filename, or path—most commonly the parent folder name. fastai comes with a number of standardized labeling methods, and ways to write your own. Here we’re telling fastai to use the <code>is_cat</code> function we just defined.</p>
<p>Finally, we define the <code>Transform</code>s that we need. A <code>Transform</code> contains code that is applied automatically during training; fastai includes many predefined <code>Transform</code>s, and adding new ones is as simple as creating a Python function. There are two kinds: <code>item_tfms</code> are applied to each item (in this case, each item is resized to a 224-pixel square), while <code>batch_tfms</code> are applied to a <em>batch</em> of items at a time using the GPU, so they’re particularly fast (we’ll see many examples of these throughout this book).</p>
<p>Why 224 pixels? This is the standard size for historical reasons (old pretrained models require this size exactly), but you can pass pretty much anything. If you increase the size, you’ll often get a model with better results (since it will be able to focus on more details), but at the price of speed and memory consumption; the opposite is true if you decrease the size.</p>
<blockquote class="blockquote">
<p>Note: Classification and Regression: <em>classification</em> and <em>regression</em> have very specific meanings in machine learning. These are the two main types of model that we will be investigating in this book. A classification model is one which attempts to predict a class, or category. That is, it’s predicting from a number of discrete possibilities, such as “dog” or “cat.” A regression model is one which attempts to predict one or more numeric quantities, such as a temperature or a location. Sometimes people use the word <em>regression</em> to refer to a particular kind of model called a <em>linear regression model</em>; this is a bad practice, and we won’t be using that terminology in this book!</p>
</blockquote>
<p>The Pet dataset contains 7,390 pictures of dogs and cats, consisting of 37 different breeds. Each image is labeled using its filename: for instance the file <em>great_pyrenees_173.jpg</em> is the 173rd example of an image of a Great Pyrenees breed dog in the dataset. The filenames start with an uppercase letter if the image is a cat, and a lowercase letter otherwise. We have to tell fastai how to get labels from the filenames, which we do by calling <code>from_name_func</code> (which means that labels can be extracted using a function applied to the filename), and passing <code>is_cat</code>, which returns <code>x[0].isupper()</code>, which evaluates to <code>True</code> if the first letter is uppercase (i.e., it’s a cat).</p>
<p>The most important parameter to mention here is <code>valid_pct=0.2</code>. This tells fastai to hold out 20% of the data and <em>not use it for training the model at all</em>. This 20% of the data is called the <em>validation set</em>; the remaining 80% is called the <em>training set</em>. The validation set is used to measure the accuracy of the model. By default, the 20% that is held out is selected randomly. The parameter <code>seed=42</code> sets the <em>random seed</em> to the same value every time we run this code, which means we get the same validation set every time we run it—this way, if we change our model and retrain it, we know that any differences are due to the changes to the model, not due to having a different random validation set.</p>
<p>fastai will <em>always</em> show you your model’s accuracy using <em>only</em> the validation set, <em>never</em> the training set. This is absolutely critical, because if you train a large enough model for a long enough time, it will eventually memorize the label of every item in your dataset! The result will not actually be a useful model, because what we care about is how well our model works on <em>previously unseen images</em>. That is always our goal when creating a model: for it to be useful on data that the model only sees in the future, after it has been trained.</p>
<p>Even when your model has not fully memorized all your data, earlier on in training it may have memorized certain parts of it. As a result, the longer you train for, the better your accuracy will get on the training set; the validation set accuracy will also improve for a while, but eventually it will start getting worse as the model starts to memorize the training set, rather than finding generalizable underlying patterns in the data. When this happens, we say that the model is <em>overfitting</em>.</p>
<p><a href="#fig-img-overfit" class="quarto-xref">Figure&nbsp;<span>1.10</span></a> shows what happens when you overfit, using a simplified example where we have just one parameter, and some randomly generated data based on the function <code>x**2</code>. As you can see, although the predictions in the overfit model are accurate for data near the observed data points, they are way off when outside of that range.</p>
<div id="fig-img-overfit" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Example of overfitting">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-img-overfit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/att_00000.png" class="img-fluid figure-img" alt="Example of overfitting" width="700">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-img-overfit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.10: Example of overfitting
</figcaption>
</figure>
</div>
<p><strong>Overfitting is the single most important and challenging issue</strong> when training for all machine learning practitioners, and all algorithms. As you will see, it is very easy to create a model that does a great job at making predictions on the exact data it has been trained on, but it is much harder to make accurate predictions on data the model has never seen before. And of course, this is the data that will actually matter in practice. For instance, if you create a handwritten digit classifier (as we will very soon!) and use it to recognize numbers written on checks, then you are never going to see any of the numbers that the model was trained on—checks will have slightly different variations of writing to deal with. You will learn many methods to avoid overfitting in this book. However, you should only use those methods after you have confirmed that overfitting is actually occurring (i.e., you have actually observed the validation accuracy getting worse during training). We often see practitioners using over-fitting avoidance techniques even when they have enough data that they didn’t need to do so, ending up with a model that may be less accurate than what they could have achieved.</p>
<blockquote class="blockquote">
<p>important: Validation Set: When you train a model, you must <em>always</em> have both a training set and a validation set, and must measure the accuracy of your model only on the validation set. If you train for too long, with not enough data, you will see the accuracy of your model start to get worse; this is called <em>overfitting</em>. fastai defaults <code>valid_pct</code> to <code>0.2</code>, so even if you forget, fastai will create a validation set for you!</p>
</blockquote>
<p>The fifth line of the code training our image recognizer tells fastai to create a <em>convolutional neural network</em> (CNN) and specifies what <em>architecture</em> to use (i.e.&nbsp;what kind of model to create), what data we want to train it on, and what <em>metric</em> to use:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> vision_learner(dls, resnet34, metrics<span class="op">=</span>error_rate)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Why a CNN? It’s the current state-of-the-art approach to creating computer vision models. We’ll be learning all about how CNNs work in this book. Their structure is inspired by how the human vision system works.</p>
<p>There are many different architectures in fastai, which we will introduce in this book (as well as discussing how to create your own). Most of the time, however, picking an architecture isn’t a very important part of the deep learning process. It’s something that academics love to talk about, but in practice it is unlikely to be something you need to spend much time on. There are some standard architectures that work most of the time, and in this case we’re using one called <em>ResNet</em> that we’ll be talking a lot about during the book; it is both fast and accurate for many datasets and problems. The <code>34</code> in <code>resnet34</code> refers to the number of layers in this variant of the architecture (other options are <code>18</code>, <code>50</code>, <code>101</code>, and <code>152</code>). Models using architectures with more layers take longer to train, and are more prone to overfitting (i.e.&nbsp;you can’t train them for as many epochs before the accuracy on the validation set starts getting worse). On the other hand, when using more data, they can be quite a bit more accurate.</p>
<p>What is a metric? A <em>metric</em> is a function that measures the quality of the model’s predictions using the validation set, and will be printed at the end of each <em>epoch</em>. In this case, we’re using <code>error_rate</code>, which is a function provided by fastai that does just what it says: tells you what percentage of images in the validation set are being classified incorrectly. Another common metric for classification is <code>accuracy</code> (which is just <code>1.0 - error_rate</code>). fastai provides many more, which will be discussed throughout this book.</p>
<p>The concept of a metric may remind you of <em>loss</em>, but there is an important distinction. The entire purpose of loss is to define a “measure of performance” that the training system can use to update weights automatically. In other words, a good choice for loss is a choice that is easy for stochastic gradient descent to use. But a metric is defined for human consumption, so a good metric is one that is easy for you to understand, and that hews as closely as possible to what you want the model to do. At times, you might decide that the loss function is a suitable metric, but that is not necessarily the case.</p>
<p><code>vision_learner</code> also has a parameter <code>pretrained</code>, which defaults to <code>True</code> (so it’s used in this case, even though we haven’t specified it), which sets the weights in your model to values that have already been trained by experts to recognize a thousand different categories across 1.3 million photos (using the famous <a href="http://www.image-net.org/"><em>ImageNet</em> dataset</a>). A model that has weights that have already been trained on some other dataset is called a <em>pretrained model</em>. You should nearly always use a pretrained model, because it means that your model, before you’ve even shown it any of your data, is already very capable. And, as you’ll see, in a deep learning model many of these capabilities are things you’ll need, almost regardless of the details of your project. For instance, parts of pretrained models will handle edge, gradient, and color detection, which are needed for many tasks.</p>
<p>When using a pretrained model, <code>vision_learner</code> will remove the last layer, since that is always specifically customized to the original training task (i.e.&nbsp;ImageNet dataset classification), and replace it with one or more new layers with randomized weights, of an appropriate size for the dataset you are working with. This last part of the model is known as the <em>head</em>.</p>
<p>Using pretrained models is the <em>most</em> important method we have to allow us to train more accurate models, more quickly, with less data, and less time and money. You might think that would mean that using pretrained models would be the most studied area in academic deep learning… but you’d be very, very wrong! The importance of pretrained models is generally not recognized or discussed in most courses, books, or software library features, and is rarely considered in academic papers. As we write this at the start of 2020, things are just starting to change, but it’s likely to take a while. So be careful: most people you speak to will probably greatly underestimate what you can do in deep learning with few resources, because they probably won’t deeply understand how to use pretrained models.</p>
<p>Using a pretrained model for a task different to what it was originally trained for is known as <em>transfer learning</em>. Unfortunately, because transfer learning is so under-studied, few domains have pretrained models available. For instance, there are currently few pretrained models available in medicine, making transfer learning challenging to use in that domain. In addition, it is not yet well understood how to use transfer learning for tasks such as time series analysis.</p>
<blockquote class="blockquote">
<p>jargon: Transfer learning: Using a pretrained model for a task different to what it was originally trained for.</p>
</blockquote>
<p>The sixth line of our code tells fastai how to <em>fit</em> the model:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>learn.fine_tune(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As we’ve discussed, the architecture only describes a <em>template</em> for a mathematical function; it doesn’t actually do anything until we provide values for the millions of parameters it contains.</p>
<p>This is the key to deep learning—determining how to fit the parameters of a model to get it to solve your problem. In order to fit a model, we have to provide at least one piece of information: how many times to look at each image (known as number of <em>epochs</em>). The number of epochs you select will largely depend on how much time you have available, and how long you find it takes in practice to fit your model. If you select a number that is too small, you can always train for more epochs later.</p>
<p>But why is the method called <code>fine_tune</code>, and not <code>fit</code>? fastai actually <em>does</em> have a method called <code>fit</code>, which does indeed fit a model (i.e.&nbsp;look at images in the training set multiple times, each time updating the parameters to make the predictions closer and closer to the target labels). But in this case, we’ve started with a pretrained model, and we don’t want to throw away all those capabilities that it already has. As you’ll learn in this book, there are some important tricks to adapt a pretrained model for a new dataset—a process called <em>fine-tuning</em>.</p>
<blockquote class="blockquote">
<p>jargon: Fine-tuning: A transfer learning technique where the parameters of a pretrained model are updated by training for additional epochs using a different task to that used for pretraining.</p>
</blockquote>
<p>When you use the <code>fine_tune</code> method, fastai will use these tricks for you. There are a few parameters you can set (which we’ll discuss later), but in the default form shown here, it does two steps:</p>
<ol type="1">
<li>Use one epoch to fit just those parts of the model necessary to get the new random head to work correctly with your dataset.</li>
<li>Use the number of epochs requested when calling the method to fit the entire model, updating the weights of the later layers (especially the head) faster than the earlier layers (which, as we’ll see, generally don’t require many changes from the pretrained weights).</li>
</ol>
<p>The <em>head</em> of a model is the part that is newly added to be specific to the new dataset. An <em>epoch</em> is one complete pass through the dataset. After calling <code>fit</code>, the results after each epoch are printed, showing the epoch number, the training and validation set losses (the “measure of performance” used for training the model), and any <em>metrics</em> you’ve requested (error rate, in this case).</p>
<p>So, with all this code our model learned to recognize cats and dogs just from labeled examples. But how did it do it?</p>
</section>
<section id="what-our-image-recognizer-learned" class="level3">
<h3 class="anchored" data-anchor-id="what-our-image-recognizer-learned">What Our Image Recognizer Learned</h3>
<p>At this stage we have an image recognizer that is working very well, but we have no idea what it is actually doing! Although many people complain that deep learning results in impenetrable “black box” models (that is, something that gives predictions but that no one can understand), this really couldn’t be further from the truth. There is a vast body of research showing how to deeply inspect deep learning models, and get rich insights from them. Having said that, all kinds of machine learning models (including deep learning, and traditional statistical models) can be challenging to fully understand, especially when considering how they will behave when coming across data that is very different to the data used to train them. We’ll be discussing this issue throughout this book.</p>
<p>In 2013 a PhD student, Matt Zeiler, and his supervisor, Rob Fergus, published the paper <a href="https://arxiv.org/pdf/1311.2901.pdf">“Visualizing and Understanding Convolutional Networks”</a>, which showed how to visualize the neural network weights learned in each layer of a model. They carefully analyzed the model that won the 2012 ImageNet competition, and used this analysis to greatly improve the model, such that they were able to go on to win the 2013 competition! <a href="#fig-img-layer1" class="quarto-xref">Figure&nbsp;<span>1.11</span></a> is the picture that they published of the first layer’s weights.</p>
<div id="fig-img-layer1" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Activations of the first layer of a CNN">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-img-layer1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/layer1.png" class="img-fluid figure-img" alt="Activations of the first layer of a CNN" width="300">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-img-layer1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.11: Activations of the first layer of a CNN (courtesy of Matthew D. Zeiler and Rob Fergus)
</figcaption>
</figure>
</div>
<p>This picture requires some explanation. For each layer, the image part with the light gray background shows the reconstructed weights pictures, and the larger section at the bottom shows the parts of the training images that most strongly matched each set of weights. For layer 1, what we can see is that the model has discovered weights that represent diagonal, horizontal, and vertical edges, as well as various different gradients. (Note that for each layer only a subset of the features are shown; in practice there are thousands across all of the layers.) These are the basic building blocks that the model has learned for computer vision. They have been widely analyzed by neuroscientists and computer vision researchers, and it turns out that these learned building blocks are very similar to the basic visual machinery in the human eye, as well as the handcrafted computer vision features that were developed prior to the days of deep learning. The next layer is represented in <a href="#fig-img-layer2" class="quarto-xref">Figure&nbsp;<span>1.12</span></a>.</p>
<div id="fig-img-layer2" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Activations of the second layer of a CNN">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-img-layer2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/layer2.png" class="img-fluid figure-img" alt="Activations of the second layer of a CNN" width="800">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-img-layer2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.12: Activations of the second layer of a CNN (courtesy of Matthew D. Zeiler and Rob Fergus)
</figcaption>
</figure>
</div>
<p>For layer 2, there are nine examples of weight reconstructions for each of the features found by the model. We can see that the model has learned to create feature detectors that look for corners, repeating lines, circles, and other simple patterns. These are built from the basic building blocks developed in the first layer. For each of these, the right-hand side of the picture shows small patches from actual images which these features most closely match. For instance, the particular pattern in row 2, column 1 matches the gradients and textures associated with sunsets.</p>
<p><a href="#fig-img-layer3" class="quarto-xref">Figure&nbsp;<span>1.13</span></a> shows the image from the paper showing the results of reconstructing the features of layer 3.</p>
<div id="fig-img-layer3" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Activations of the third layer of a CNN">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-img-layer3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/chapter2_layer3.PNG" class="img-fluid figure-img" alt="Activations of the third layer of a CNN" width="800">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-img-layer3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.13: Activations of the third layer of a CNN (courtesy of Matthew D. Zeiler and Rob Fergus)
</figcaption>
</figure>
</div>
<p>As you can see by looking at the righthand side of this picture, the features are now able to identify and match with higher-level semantic components, such as car wheels, text, and flower petals. Using these components, layers four and five can identify even higher-level concepts, as shown in <a href="#fig-img-layer4" class="quarto-xref">Figure&nbsp;<span>1.14</span></a>.</p>
<div id="fig-img-layer4" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Activations of layers 4 and 5 of a CNN">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-img-layer4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/chapter2_layer4and5.PNG" class="img-fluid figure-img" alt="Activations of layers 4 and 5 of a CNN" width="800">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-img-layer4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.14: Activations of layers 4 and 5 of a CNN (courtesy of Matthew D. Zeiler and Rob Fergus)
</figcaption>
</figure>
</div>
<p>This article was studying an older model called <em>AlexNet</em> that only contained five layers. Networks developed since then can have hundreds of layers—so you can imagine how rich the features developed by these models can be!</p>
<p>When we fine-tuned our pretrained model earlier, we adapted what those last layers focus on (flowers, humans, animals) to specialize on the cats versus dogs problem. More generally, we could specialize such a pretrained model on many different tasks. Let’s have a look at some examples.</p>
</section>
<section id="image-recognizers-can-tackle-non-image-tasks" class="level3">
<h3 class="anchored" data-anchor-id="image-recognizers-can-tackle-non-image-tasks">Image Recognizers Can Tackle Non-Image Tasks</h3>
<p>An image recognizer can, as its name suggests, only recognize images. But a lot of things can be represented as images, which means that an image recogniser can learn to complete many tasks.</p>
<p>For instance, a sound can be converted to a spectrogram, which is a chart that shows the amount of each frequency at each time in an audio file. Fast.ai student Ethan Sutin used this approach to easily beat the published accuracy of a state-of-the-art <a href="https://medium.com/@etown/great-results-on-audio-classification-with-fastai-library-ccaf906c5f52">environmental sound detection model</a> using a dataset of 8,732 urban sounds. fastai’s <code>show_batch</code> clearly shows how each different sound has a quite distinctive spectrogram, as you can see in <a href="#fig-img-spect" class="quarto-xref">Figure&nbsp;<span>1.15</span></a>.</p>
<div id="fig-img-spect" class="quarto-float quarto-figure quarto-figure-center anchored" alt="show_batch with spectrograms of sounds">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-img-spect-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/att_00012.png" class="img-fluid figure-img" alt="show_batch with spectrograms of sounds" width="400">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-img-spect-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.15: show_batch with spectrograms of sounds
</figcaption>
</figure>
</div>
<p>A time series can easily be converted into an image by simply plotting the time series on a graph. However, it is often a good idea to try to represent your data in a way that makes it as easy as possible to pull out the most important components. In a time series, things like seasonality and anomalies are most likely to be of interest. There are various transformations available for time series data. For instance, fast.ai student Ignacio Oguiza created images from a time series dataset for olive oil classification, using a technique called Gramian Angular Difference Field (GADF); you can see the result in <a href="#fig-ts-image" class="quarto-xref">Figure&nbsp;<span>1.16</span></a>. He then fed those images to an image classification model just like the one you see in this chapter. His results, despite having only 30 training set images, were well over 90% accurate, and close to the state of the art.</p>
<div id="fig-ts-image" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Converting a time series into an image">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ts-image-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/att_00013.png" class="img-fluid figure-img" alt="Converting a time series into an image" width="700">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ts-image-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.16: Converting a time series into an image
</figcaption>
</figure>
</div>
<p>Another interesting fast.ai student project example comes from Gleb Esman. He was working on fraud detection at Splunk, using a dataset of users’ mouse movements and mouse clicks. He turned these into pictures by drawing an image where the position, speed, and acceleration of the mouse pointer was displayed using coloured lines, and the clicks were displayed using <a href="https://www.splunk.com/en_us/blog/security/deep-learning-with-splunk-and-tensorflow-for-security-catching-the-fraudster-in-neural-networks-with-behavioral-biometrics.html">small colored circles</a>, as shown in <a href="#fig-splunk" class="quarto-xref">Figure&nbsp;<span>1.17</span></a>. He then fed this into an image recognition model just like the one we’ve used in this chapter, and it worked so well that it led to a patent for this approach to fraud analytics!</p>
<div id="fig-splunk" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Converting computer mouse behavior to an image">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-splunk-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/att_00014.png" class="img-fluid figure-img" alt="Converting computer mouse behavior to an image" width="450">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-splunk-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.17: Converting computer mouse behavior to an image
</figcaption>
</figure>
</div>
<p>Another example comes from the paper <a href="https://ieeexplore.ieee.org/abstract/document/8328749">“Malware Classification with Deep Convolutional Neural Networks”</a> by Mahmoud Kalash et al., which explains that “the malware binary file is divided into 8-bit sequences which are then converted to equivalent decimal values. This decimal vector is reshaped and a gray-scale image is generated that represents the malware sample,” like in <a href="#fig-malware-proc" class="quarto-xref">Figure&nbsp;<span>1.18</span></a>.</p>
<div id="fig-malware-proc" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Malware classification process">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-malware-proc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/att_00055.png" class="img-fluid figure-img" alt="Malware classification process" width="623">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-malware-proc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.18: Malware classification process
</figcaption>
</figure>
</div>
<p>The authors then show “pictures” generated through this process of malware in different categories, as shown in <a href="#fig-malware-eg" class="quarto-xref">Figure&nbsp;<span>1.19</span></a>.</p>
<div id="fig-malware-eg" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Malware examples">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-malware-eg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/att_00056.png" class="img-fluid figure-img" alt="Malware examples" width="650">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-malware-eg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.19: Malware examples
</figcaption>
</figure>
</div>
<p>As you can see, the different types of malware look very distinctive to the human eye. The model the researchers trained based on this image representation was more accurate at malware classification than any previous approach shown in the academic literature. This suggests a good rule of thumb for converting a dataset into an image representation: if the human eye can recognize categories from the images, then a deep learning model should be able to do so too.</p>
<p>In general, you’ll find that a small number of general approaches in deep learning can go a long way, if you’re a bit creative in how you represent your data! You shouldn’t think of approaches like the ones described here as “hacky workarounds,” because actually they often (as here) beat previously state-of-the-art results. These really are the right ways to think about these problem domains.</p>
</section>
<section id="jargon-recap" class="level3">
<h3 class="anchored" data-anchor-id="jargon-recap">Jargon Recap</h3>
<p>We just covered a lot of information so let’s recap briefly, <a href="#tbl-dljargon" class="quarto-xref">Table&nbsp;<span>1.2</span></a> provides a handy vocabulary.</p>
<div id="tbl-dljargon" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-dljargon-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1.2: Deep learning vocabulary
</figcaption>
<div aria-describedby="tbl-dljargon-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 75%">
</colgroup>
<thead>
<tr class="header">
<th>Term</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Label</td>
<td>The data that we’re trying to predict, such as “dog” or “cat”</td>
</tr>
<tr class="even">
<td>Architecture</td>
<td>The <em>template</em> of the model that we’re trying to fit; the actual mathematical function that we’re passing the input data and parameters to</td>
</tr>
<tr class="odd">
<td>Model</td>
<td>The combination of the architecture with a particular set of parameters</td>
</tr>
<tr class="even">
<td>Parameters</td>
<td>The values in the model that change what task it can do, and are updated through model training</td>
</tr>
<tr class="odd">
<td>Fit</td>
<td>Update the parameters of the model such that the predictions of the model using the input data match the target labels</td>
</tr>
<tr class="even">
<td>Train</td>
<td>A synonym for <em>fit</em></td>
</tr>
<tr class="odd">
<td>Pretrained model</td>
<td>A model that has already been trained, generally using a large dataset, and will be fine-tuned</td>
</tr>
<tr class="even">
<td>Fine-tune</td>
<td>Update a pretrained model for a different task</td>
</tr>
<tr class="odd">
<td>Epoch</td>
<td>One complete pass through the input data</td>
</tr>
<tr class="even">
<td>Loss</td>
<td>A measure of how good the model is, chosen to drive training via SGD</td>
</tr>
<tr class="odd">
<td>Metric</td>
<td>A measurement of how good the model is, using the validation set, chosen for human consumption</td>
</tr>
<tr class="even">
<td>Validation set</td>
<td>A set of data held out from training, used only for measuring how good the model is</td>
</tr>
<tr class="odd">
<td>Training set</td>
<td>The data used for fitting the model; does not include any data from the validation set</td>
</tr>
<tr class="even">
<td>Overfitting</td>
<td>Training a model in such a way that it <em>remembers</em> specific features of the input data, rather than generalizing well to data not seen during training</td>
</tr>
<tr class="odd">
<td>CNN</td>
<td>Convolutional neural network; a type of neural network that works particularly well for computer vision tasks</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>With this vocabulary in hand, we are now in a position to bring together all the key concepts introduced so far. Take a moment to review those definitions and read the following summary. If you can follow the explanation, then you’re well equipped to understand the discussions to come.</p>
<p><em>Machine learning</em> is a discipline where we define a program not by writing it entirely ourselves, but by learning from data. <em>Deep learning</em> is a specialty within machine learning that uses <em>neural networks</em> with multiple <em>layers</em>. <em>Image classification</em> is a representative example (also known as <em>image recognition</em>). We start with <em>labeled data</em>; that is, a set of images where we have assigned a <em>label</em> to each image indicating what it represents. Our goal is to produce a program, called a <em>model</em>, which, given a new image, will make an accurate <em>prediction</em> regarding what that new image represents.</p>
<p>Every model starts with a choice of <em>architecture</em>, a general template for how that kind of model works internally. The process of <em>training</em> (or <em>fitting</em>) the model is the process of finding a set of <em>parameter values</em> (or <em>weights</em>) that specialize that general architecture into a model that works well for our particular kind of data. In order to define how well a model does on a single prediction, we need to define a <em>loss function</em>, which determines how we score a prediction as good or bad.</p>
<p>To make the training process go faster, we might start with a <em>pretrained model</em>—a model that has already been trained on someone else’s data. We can then adapt it to our data by training it a bit more on our data, a process called <em>fine-tuning</em>.</p>
<p>When we train a model, a key concern is to ensure that our model <em>generalizes</em>—that is, that it learns general lessons from our data which also apply to new items it will encounter, so that it can make good predictions on those items. The risk is that if we train our model badly, instead of learning general lessons it effectively memorizes what it has already seen, and then it will make poor predictions about new images. Such a failure is called <em>overfitting</em>. In order to avoid this, we always divide our data into two parts, the <em>training set</em> and the <em>validation set</em>. We train the model by showing it only the training set and then we evaluate how well the model is doing by seeing how well it performs on items from the validation set. In this way, we check if the lessons the model learns from the training set are lessons that generalize to the validation set. In order for a person to assess how well the model is doing on the validation set overall, we define a <em>metric</em>. During the training process, when the model has seen every item in the training set, we call that an <em>epoch</em>.</p>
<p>All these concepts apply to machine learning in general. That is, they apply to all sorts of schemes for defining a model by training it with data. What makes deep learning distinctive is a particular class of architectures: the architectures based on <em>neural networks</em>. In particular, tasks like image classification rely heavily on <em>convolutional neural networks</em>, which we will discuss shortly.</p>
</section>
</section>
<section id="deep-learning-is-not-just-for-image-classification" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="deep-learning-is-not-just-for-image-classification"><span class="header-section-number">1.7</span> Deep Learning Is Not Just for Image Classification</h2>
<p>Deep learning’s effectiveness for classifying images has been widely discussed in recent years, even showing <em>superhuman</em> results on complex tasks like recognizing malignant tumors in CT scans. But it can do a lot more than this, as we will show here.</p>
<p>For instance, let’s talk about something that is critically important for autonomous vehicles: localizing objects in a picture. If a self-driving car doesn’t know where a pedestrian is, then it doesn’t know how to avoid one! Creating a model that can recognize the content of every individual pixel in an image is called <em>segmentation</em>. Here is how we can train a segmentation model with fastai, using a subset of the <a href="http://www0.cs.ucl.ac.uk/staff/G.Brostow/papers/Brostow_2009-PRL.pdf"><em>Camvid</em> dataset</a> from the paper “Semantic Object Classes in Video: A High-Definition Ground Truth Database” by Gabruel J. Brostow, Julien Fauqueur, and Roberto Cipolla:</p>
<div id="cell-141" class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> untar_data(URLs.CAMVID_TINY)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> SegmentationDataLoaders.from_label_func(</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    path, bs<span class="op">=</span><span class="dv">8</span>, fnames <span class="op">=</span> get_image_files(path<span class="op">/</span><span class="st">"images"</span>),</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    label_func <span class="op">=</span> <span class="kw">lambda</span> o: path<span class="op">/</span><span class="st">'labels'</span><span class="op">/</span><span class="ss">f'</span><span class="sc">{</span>o<span class="sc">.</span>stem<span class="sc">}</span><span class="ss">_P</span><span class="sc">{</span>o<span class="sc">.</span>suffix<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    codes <span class="op">=</span> np.loadtxt(path<span class="op">/</span><span class="st">'codes.txt'</span>, dtype<span class="op">=</span><span class="bu">str</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> unet_learner(dls, resnet34)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>learn.fine_tune(<span class="dv">8</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>2.641862</td>
<td>2.140568</td>
<td>00:02</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1.624964</td>
<td>1.464210</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>1</td>
<td>1.454148</td>
<td>1.284032</td>
<td>00:02</td>
</tr>
<tr class="odd">
<td>2</td>
<td>1.342955</td>
<td>1.048562</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>3</td>
<td>1.199765</td>
<td>0.852787</td>
<td>00:02</td>
</tr>
<tr class="odd">
<td>4</td>
<td>1.078090</td>
<td>0.838206</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>5</td>
<td>0.975496</td>
<td>0.746806</td>
<td>00:02</td>
</tr>
<tr class="odd">
<td>6</td>
<td>0.892793</td>
<td>0.725384</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>7</td>
<td>0.827645</td>
<td>0.726778</td>
<td>00:02</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>We are not even going to walk through this code line by line, because it is nearly identical to our previous example! (Although we will be doing a deep dive into segmentation models in <a href="book15.html" class="quarto-xref"><span>Chapter 15</span></a>, along with all of the other models that we are briefly introducing in this chapter, and many, many more.)</p>
<p>We can visualize how well it achieved its task, by asking the model to color-code each pixel of an image. As you can see, it nearly perfectly classifies every pixel in every object. For instance, notice that all of the cars are overlaid with the same color and all of the trees are overlaid with the same color (in each pair of images, the lefthand image is the ground truth label and the right is the prediction from the model):</p>
<div id="cell-143" class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>learn.show_results(max_n<span class="op">=</span><span class="dv">6</span>, figsize<span class="op">=</span>(<span class="dv">7</span>,<span class="dv">8</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="intro_files/figure-html/cell-16-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>One other area where deep learning has dramatically improved in the last couple of years is natural language processing (NLP). Computers can now generate text, translate automatically from one language to another, analyze comments, label words in sentences, and much more. Here is all of the code necessary to train a model that can classify the sentiment of a movie review better than anything that existed in the world just five years ago:</p>
<div id="cell-145" class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastai.text.<span class="bu">all</span> <span class="im">import</span> <span class="op">*</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid<span class="op">=</span><span class="st">'test'</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> text_classifier_learner(dls, AWD_LSTM, drop_mult<span class="op">=</span><span class="fl">0.5</span>, metrics<span class="op">=</span>accuracy)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>learn.fine_tune(<span class="dv">4</span>, <span class="fl">1e-2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.878776</td>
<td>0.748753</td>
<td>0.500400</td>
<td>01:27</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.679118</td>
<td>0.674778</td>
<td>0.584040</td>
<td>02:45</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.653671</td>
<td>0.670396</td>
<td>0.618040</td>
<td>02:55</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.598665</td>
<td>0.551815</td>
<td>0.718920</td>
<td>05:28</td>
</tr>
<tr class="even">
<td>3</td>
<td>0.556812</td>
<td>0.507450</td>
<td>0.752480</td>
<td>03:11</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>#clean If you hit a “CUDA out of memory error” after running this cell, click on the menu Kernel, then restart. Instead of executing the cell above, copy and paste the following code in it:</p>
<pre><code>from fastai.text.all import *

dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test', bs=32)
learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)
learn.fine_tune(4, 1e-2)</code></pre>
<p>This reduces the batch size to 32 (we will explain this later). If you keep hitting the same error, change 32 to 16.</p>
<p>This model is using the <a href="https://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf">“IMDb Large Movie Review dataset”</a> from the paper “Learning Word Vectors for Sentiment Analysis” by Andrew Maas et al.&nbsp;It works well with movie reviews of many thousands of words, but let’s test it out on a very short one to see how it does its thing:</p>
<div id="cell-148" class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>learn.predict(<span class="st">"I really liked that movie!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<pre><code>('neg', tensor(0), tensor([0.8786, 0.1214]))</code></pre>
</div>
</div>
<p>Here we can see the model has considered the review to be positive. The second part of the result is the index of “pos” in our data vocabulary and the last part is the probabilities attributed to each class (99.6% for “pos” and 0.4% for “neg”).</p>
<p>Now it’s your turn! Write your own mini movie review, or copy one from the internet, and you can see what this model thinks about it.</p>
<section id="sidebar-the-order-matters" class="level3">
<h3 class="anchored" data-anchor-id="sidebar-the-order-matters">Sidebar: The Order Matters</h3>
<p>In a Jupyter notebook, the order in which you execute each cell is very important. It’s not like Excel, where everything gets updated as soon as you type something anywhere—it has an inner state that gets updated each time you execute a cell. For instance, when you run the first cell of the notebook (with the “CLICK ME” comment), you create an object called <code>learn</code> that contains a model and data for an image classification problem. If we were to run the cell just shown in the text (the one that predicts if a review is good or not) straight after, we would get an error as this <code>learn</code> object does not contain a text classification model. This cell needs to be run after the one containing:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastai.text.<span class="bu">all</span> <span class="im">import</span> <span class="op">*</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid<span class="op">=</span><span class="st">'test'</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> text_classifier_learner(dls, AWD_LSTM, drop_mult<span class="op">=</span><span class="fl">0.5</span>, </span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>                                metrics<span class="op">=</span>accuracy)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>learn.fine_tune(<span class="dv">4</span>, <span class="fl">1e-2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The outputs themselves can be deceiving, because they include the results of the last time the cell was executed; if you change the code inside a cell without executing it, the old (misleading) results will remain.</p>
<p>Except when we mention it explicitly, the notebooks provided on the <a href="https://book.fast.ai/">book website</a> are meant to be run in order, from top to bottom. In general, when experimenting, you will find yourself executing cells in any order to go fast (which is a super neat feature of Jupyter Notebook), but once you have explored and arrived at the final version of your code, make sure you can run the cells of your notebooks in order (your future self won’t necessarily remember the convoluted path you took otherwise!).</p>
<p>In command mode, pressing <code>0</code> twice will restart the <em>kernel</em> (which is the engine powering your notebook). This will wipe your state clean and make it as if you had just started in the notebook. Choose Run All Above from the Cell menu to run all cells above the point where you are. We have found this to be very useful when developing the fastai library.</p>
</section>
<section id="end-sidebar-1" class="level3">
<h3 class="anchored" data-anchor-id="end-sidebar-1">End sidebar</h3>
<p>If you ever have any questions about a fastai method, you should use the function <code>doc</code>, passing it the method name:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>doc(learn.predict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This will make a small window pop up with content like this:</p>
<p><img src="images/doc_ex.png" class="img-fluid" width="600"></p>
<p>A brief one-line explanation is provided by <code>doc</code>. The “Show in docs” link takes you to the full documentation, where you’ll find all the details and lots of examples. Also, most of fastai’s methods are just a handful of lines, so you can click the “source” link to see exactly what’s going on behind the scenes.</p>
<p>Let’s move on to something much less sexy, but perhaps significantly more widely commercially useful: building models from plain <em>tabular</em> data.</p>
<blockquote class="blockquote">
<p>jargon: Tabular: Data that is in the form of a table, such as from a spreadsheet, database, or CSV file. A tabular model is a model that tries to predict one column of a table based on information in other columns of the table.</p>
</blockquote>
<p>It turns out that looks very similar too. Here is the code necessary to train a model that will predict whether a person is a high-income earner, based on their socioeconomic background:</p>
<div id="cell-157" class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastai.tabular.<span class="bu">all</span> <span class="im">import</span> <span class="op">*</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> untar_data(URLs.ADULT_SAMPLE)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> TabularDataLoaders.from_csv(path<span class="op">/</span><span class="st">'adult.csv'</span>, path<span class="op">=</span>path, y_names<span class="op">=</span><span class="st">"salary"</span>,</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    cat_names <span class="op">=</span> [<span class="st">'workclass'</span>, <span class="st">'education'</span>, <span class="st">'marital-status'</span>, <span class="st">'occupation'</span>,</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>                 <span class="st">'relationship'</span>, <span class="st">'race'</span>],</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    cont_names <span class="op">=</span> [<span class="st">'age'</span>, <span class="st">'fnlwgt'</span>, <span class="st">'education-num'</span>],</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    procs <span class="op">=</span> [Categorify, FillMissing, Normalize])</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> tabular_learner(dls, metrics<span class="op">=</span>accuracy)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As you see, we had to tell fastai which columns are <em>categorical</em> (that is, contain values that are one of a discrete set of choices, such as <code>occupation</code>) and which are <em>continuous</em> (that is, contain a number that represents a quantity, such as <code>age</code>).</p>
<p>There is no pretrained model available for this task (in general, pretrained models are not widely available for any tabular modeling tasks, although some organizations have created them for internal use), so we don’t use <code>fine_tune</code> in this case. Instead we use <code>fit_one_cycle</code>, the most commonly used method for training fastai models <em>from scratch</em> (i.e.&nbsp;without transfer learning):</p>
<div id="cell-159" class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.372397</td>
<td>0.357177</td>
<td>0.832463</td>
<td>00:08</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.351544</td>
<td>0.341505</td>
<td>0.841523</td>
<td>00:08</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.338763</td>
<td>0.339184</td>
<td>0.845670</td>
<td>00:08</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>This model is using the <a href="http://robotics.stanford.edu/~ronnyk/nbtree.pdf"><em>Adult</em> dataset</a>, from the paper “Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid” by Rob Kohavi, which contains some demographic data about individuals (like their education, marital status, race, sex, and whether or not they have an annual income greater than $50k). The model is over 80% accurate, and took around 30 seconds to train.</p>
<p>Let’s look at one more. Recommendation systems are very important, particularly in e-commerce. Companies like Amazon and Netflix try hard to recommend products or movies that users might like. Here’s how to train a model that will predict movies people might like, based on their previous viewing habits, using the <a href="https://doi.org/10.1145/2827872">MovieLens dataset</a>:</p>
<div id="cell-162" class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastai.collab <span class="im">import</span> <span class="op">*</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> untar_data(URLs.ML_SAMPLE)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> CollabDataLoaders.from_csv(path<span class="op">/</span><span class="st">'ratings.csv'</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> collab_learner(dls, y_range<span class="op">=</span>(<span class="fl">0.5</span>,<span class="fl">5.5</span>))</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>learn.fine_tune(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1.510897</td>
<td>1.410028</td>
<td>00:00</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1.375435</td>
<td>1.350930</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>1</td>
<td>1.270062</td>
<td>1.173962</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>2</td>
<td>1.023159</td>
<td>0.879298</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>3</td>
<td>0.797398</td>
<td>0.739787</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>4</td>
<td>0.685500</td>
<td>0.700903</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>5</td>
<td>0.646508</td>
<td>0.686387</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>6</td>
<td>0.623985</td>
<td>0.681087</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>7</td>
<td>0.606319</td>
<td>0.676885</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>8</td>
<td>0.606975</td>
<td>0.675833</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>9</td>
<td>0.602670</td>
<td>0.675682</td>
<td>00:00</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>This model is predicting movie ratings on a scale of 0.5 to 5.0 to within around 0.6 average error. Since we’re predicting a continuous number, rather than a category, we have to tell fastai what range our target has, using the <code>y_range</code> parameter.</p>
<p>Although we’re not actually using a pretrained model (for the same reason that we didn’t for the tabular model), this example shows that fastai lets us use <code>fine_tune</code> anyway in this case (you’ll learn how and why this works in <a href="book5.html" class="quarto-xref"><span>Chapter 5</span></a>). Sometimes it’s best to experiment with <code>fine_tune</code> versus <code>fit_one_cycle</code> to see which works best for your dataset.</p>
<p>We can use the same <code>show_results</code> call we saw earlier to view a few examples of user and movie IDs, actual ratings, and predictions:</p>
<div id="cell-164" class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>learn.show_results()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">userId</th>
<th data-quarto-table-cell-role="th">movieId</th>
<th data-quarto-table-cell-role="th">rating</th>
<th data-quarto-table-cell-role="th">rating_pred</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>66.0</td>
<td>79.0</td>
<td>4.0</td>
<td>3.978900</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>97.0</td>
<td>15.0</td>
<td>4.0</td>
<td>3.851795</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>55.0</td>
<td>79.0</td>
<td>3.5</td>
<td>3.945623</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>98.0</td>
<td>91.0</td>
<td>4.0</td>
<td>4.458704</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>53.0</td>
<td>7.0</td>
<td>5.0</td>
<td>4.670005</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>26.0</td>
<td>69.0</td>
<td>5.0</td>
<td>4.319870</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>81.0</td>
<td>16.0</td>
<td>4.5</td>
<td>4.426761</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>80.0</td>
<td>7.0</td>
<td>4.0</td>
<td>4.046183</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>51.0</td>
<td>94.0</td>
<td>5.0</td>
<td>3.499996</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="sidebar-datasets-food-for-models" class="level3">
<h3 class="anchored" data-anchor-id="sidebar-datasets-food-for-models">Sidebar: Datasets: Food for Models</h3>
<p>You’ve already seen quite a few models in this section, each one trained using a different dataset to do a different task. In machine learning and deep learning, we can’t do anything without data. So, the people that create datasets for us to train our models on are the (often underappreciated) heroes. Some of the most useful and important datasets are those that become important <em>academic baselines</em>; that is, datasets that are widely studied by researchers and used to compare algorithmic changes. Some of these become household names (at least, among households that train models!), such as MNIST, CIFAR-10, and ImageNet.</p>
<p>The datasets used in this book have been selected because they provide great examples of the kinds of data that you are likely to encounter, and the academic literature has many examples of model results using these datasets to which you can compare your work.</p>
<p>Most datasets used in this book took the creators a lot of work to build. For instance, later in the book we’ll be showing you how to create a model that can translate between French and English. The key input to this is a French/English parallel text corpus prepared back in 2009 by Professor Chris Callison-Burch of the University of Pennsylvania. This dataset contains over 20 million sentence pairs in French and English. He built the dataset in a really clever way: by crawling millions of Canadian web pages (which are often multilingual) and then using a set of simple heuristics to transform URLs of French content onto URLs pointing to the same content in English.</p>
<p>As you look at datasets throughout this book, think about where they might have come from, and how they might have been curated. Then think about what kinds of interesting datasets you could create for your own projects. (We’ll even take you step by step through the process of creating your own image dataset soon.)</p>
<p>fast.ai has spent a lot of time creating cut-down versions of popular datasets that are specially designed to support rapid prototyping and experimentation, and to be easier to learn with. In this book we will often start by using one of the cut-down versions and later scale up to the full-size version (just as we’re doing in this chapter!). In fact, this is how the world’s top practitioners do their modeling in practice; they do most of their experimentation and prototyping with subsets of their data, and only use the full dataset when they have a good understanding of what they have to do.</p>
</section>
<section id="end-sidebar-2" class="level3">
<h3 class="anchored" data-anchor-id="end-sidebar-2">End sidebar</h3>
<p>Each of the models we trained showed a training and validation loss. A good validation set is one of the most important pieces of the training process. Let’s see why and learn how to create one.</p>
</section>
</section>
<section id="validation-sets-and-test-sets" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="validation-sets-and-test-sets"><span class="header-section-number">1.8</span> Validation Sets and Test Sets</h2>
<p>As we’ve discussed, the goal of a model is to make predictions about data. But the model training process is fundamentally dumb. If we trained a model with all our data, and then evaluated the model using that same data, we would not be able to tell how well our model can perform on data it hasn’t seen. Without this very valuable piece of information to guide us in training our model, there is a very good chance it would become good at making predictions about that data but would perform poorly on new data.</p>
<p>To avoid this, our first step was to split our dataset into two sets: the <em>training set</em> (which our model sees in training) and the <em>validation set</em>, also known as the <em>development set</em> (which is used only for evaluation). This lets us test that the model learns lessons from the training data that generalize to new data, the validation data.</p>
<p>One way to understand this situation is that, in a sense, we don’t want our model to get good results by “cheating.” If it makes an accurate prediction for a data item, that should be because it has learned characteristics of that kind of item, and not because the model has been shaped by <em>actually having seen that particular item</em>.</p>
<p>Splitting off our validation data means our model never sees it in training and so is completely untainted by it, and is not cheating in any way. Right?</p>
<p>In fact, not necessarily. The situation is more subtle. This is because in realistic scenarios we rarely build a model just by training its weight parameters once. Instead, we are likely to explore many versions of a model through various modeling choices regarding network architecture, learning rates, data augmentation strategies, and other factors we will discuss in upcoming chapters. Many of these choices can be described as choices of <em>hyperparameters</em>. The word reflects that they are parameters about parameters, since they are the higher-level choices that govern the meaning of the weight parameters.</p>
<p>The problem is that even though the ordinary training process is only looking at predictions on the training data when it learns values for the weight parameters, the same is not true of us. We, as modelers, are evaluating the model by looking at predictions on the validation data when we decide to explore new hyperparameter values! So subsequent versions of the model are, indirectly, shaped by us having seen the validation data. Just as the automatic training process is in danger of overfitting the training data, we are in danger of overfitting the validation data through human trial and error and exploration.</p>
<p>The solution to this conundrum is to introduce another level of even more highly reserved data, the <em>test set</em>. Just as we hold back the validation data from the training process, we must hold back the test set data even from ourselves. It cannot be used to improve the model; it can only be used to evaluate the model at the very end of our efforts. In effect, we define a hierarchy of cuts of our data, based on how fully we want to hide it from training and modeling processes: training data is fully exposed, the validation data is less exposed, and test data is totally hidden. This hierarchy parallels the different kinds of modeling and evaluation processes themselves—the automatic training process with back propagation, the more manual process of trying different hyper-parameters between training sessions, and the assessment of our final result.</p>
<p>The test and validation sets should have enough data to ensure that you get a good estimate of your accuracy. If you’re creating a cat detector, for instance, you generally want at least 30 cats in your validation set. That means that if you have a dataset with thousands of items, using the default 20% validation set size may be more than you need. On the other hand, if you have lots of data, using some of it for validation probably doesn’t have any downsides.</p>
<p>Having two levels of “reserved data”—a validation set and a test set, with one level representing data that you are virtually hiding from yourself—may seem a bit extreme. But the reason it is often necessary is because models tend to gravitate toward the simplest way to do good predictions (memorization), and we as fallible humans tend to gravitate toward fooling ourselves about how well our models are performing. The discipline of the test set helps us keep ourselves intellectually honest. That doesn’t mean we <em>always</em> need a separate test set—if you have very little data, you may need to just have a validation set—but generally it’s best to use one if at all possible.</p>
<p>This same discipline can be critical if you intend to hire a third party to perform modeling work on your behalf. A third party might not understand your requirements accurately, or their incentives might even encourage them to misunderstand them. A good test set can greatly mitigate these risks and let you evaluate whether their work solves your actual problem.</p>
<p>To put it bluntly, if you’re a senior decision maker in your organization (or you’re advising senior decision makers), the most important takeaway is this: if you ensure that you really understand what test and validation sets are and why they’re important, then you’ll avoid the single biggest source of failures we’ve seen when organizations decide to use AI. For instance, if you’re considering bringing in an external vendor or service, make sure that you hold out some test data that the vendor <em>never gets to see</em>. Then <em>you</em> check their model on your test data, using a metric that <em>you</em> choose based on what actually matters to you in practice, and <em>you</em> decide what level of performance is adequate. (It’s also a good idea for you to try out some simple baseline yourself, so you know what a really simple model can achieve. Often it’ll turn out that your simple model performs just as well as one produced by an external “expert”!)</p>
<section id="use-judgment-in-defining-test-sets" class="level3">
<h3 class="anchored" data-anchor-id="use-judgment-in-defining-test-sets">Use Judgment in Defining Test Sets</h3>
<p>To do a good job of defining a validation set (and possibly a test set), you will sometimes want to do more than just randomly grab a fraction of your original dataset. Remember: a key property of the validation and test sets is that they must be representative of the new data you will see in the future. This may sound like an impossible order! By definition, you haven’t seen this data yet. But you usually still do know some things.</p>
<p>It’s instructive to look at a few example cases. Many of these examples come from predictive modeling competitions on the <a href="https://www.kaggle.com/">Kaggle</a> platform, which is a good representation of problems and methods you might see in practice.</p>
<p>One case might be if you are looking at time series data. For a time series, choosing a random subset of the data will be both too easy (you can look at the data both before and after the dates you are trying to predict) and not representative of most business use cases (where you are using historical data to build a model for use in the future). If your data includes the date and you are building a model to use in the future, you will want to choose a continuous section with the latest dates as your validation set (for instance, the last two weeks or last month of available data).</p>
<p>Suppose you want to split the time series data in <a href="#fig-timeseries1" class="quarto-xref">Figure&nbsp;<span>1.20</span></a> into training and validation sets.</p>
<div id="fig-timeseries1" class="quarto-float quarto-figure quarto-figure-center anchored" alt="A serie of values">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-timeseries1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/timeseries1.png" class="img-fluid figure-img" alt="A serie of values" width="400">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-timeseries1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.20: A time series
</figcaption>
</figure>
</div>
<p>A random subset is a poor choice (too easy to fill in the gaps, and not indicative of what you’ll need in production), as we can see in <a href="#fig-timeseries2" class="quarto-xref">Figure&nbsp;<span>1.21</span></a>.</p>
<div id="fig-timeseries2" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Random training subset">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-timeseries2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/timeseries2.png" class="img-fluid figure-img" alt="Random training subset" width="400">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-timeseries2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.21: A poor training subset
</figcaption>
</figure>
</div>
<p>Instead, use the earlier data as your training set (and the later data for the validation set), as shown in <a href="#fig-timeseries3" class="quarto-xref">Figure&nbsp;<span>1.22</span></a>.</p>
<div id="fig-timeseries3" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Training subset using the data up to a certain timestamp">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-timeseries3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/timeseries3.png" class="img-fluid figure-img" alt="Training subset using the data up to a certain timestamp" width="400">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-timeseries3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.22: A good training subset
</figcaption>
</figure>
</div>
<p>For example, Kaggle had a competition to <a href="https://www.kaggle.com/c/favorita-grocery-sales-forecasting">predict the sales in a chain of Ecuadorian grocery stores</a>. Kaggle’s training data ran from Jan 1 2013 to Aug 15 2017, and the test data spanned Aug 16 2017 to Aug 31 2017. That way, the competition organizer ensured that entrants were making predictions for a time period that was <em>in the future</em>, from the perspective of their model. This is similar to the way quant hedge fund traders do <em>back-testing</em> to check whether their models are predictive of future periods, based on past data.</p>
<p>A second common case is when you can easily anticipate ways the data you will be making predictions for in production may be <em>qualitatively different</em> from the data you have to train your model with.</p>
<p>In the Kaggle <a href="https://www.kaggle.com/c/state-farm-distracted-driver-detection">distracted driver competition</a>, the independent variables are pictures of drivers at the wheel of a car, and the dependent variables are categories such as texting, eating, or safely looking ahead. Lots of pictures are of the same drivers in different positions, as we can see in <a href="#fig-img-driver" class="quarto-xref">Figure&nbsp;<span>1.23</span></a>. If you were an insurance company building a model from this data, note that you would be most interested in how the model performs on drivers it hasn’t seen before (since you would likely have training data only for a small group of people). In recognition of this, the test data for the competition consists of images of people that don’t appear in the training set.</p>
<div id="fig-img-driver" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Two pictures from the training data, showing the same driver">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-img-driver-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/driver.PNG" class="img-fluid figure-img" alt="Two pictures from the training data, showing the same driver" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-img-driver-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.23: Two pictures from the training data
</figcaption>
</figure>
</div>
<p>If you put one of the images in <a href="#fig-img-driver" class="quarto-xref">Figure&nbsp;<span>1.23</span></a> in your training set and one in the validation set, your model will have an easy time making a prediction for the one in the validation set, so it will seem to be performing better than it would on new people. Another perspective is that if you used all the people in training your model, your model might be overfitting to particularities of those specific people, and not just learning the states (texting, eating, etc.).</p>
<p>A similar dynamic was at work in the <a href="https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring">Kaggle fisheries competition</a> to identify the species of fish caught by fishing boats in order to reduce illegal fishing of endangered populations. The test set consisted of boats that didn’t appear in the training data. This means that you’d want your validation set to include boats that are not in the training set.</p>
<p>Sometimes it may not be clear how your validation data will differ. For instance, for a problem using satellite imagery, you’d need to gather more information on whether the training set just contained certain geographic locations, or if it came from geographically scattered data.</p>
<p>Now that you have gotten a taste of how to build a model, you can decide what you want to dig into next.</p>
</section>
</section>
<section id="a-choose-your-own-adventure-moment" class="level2" data-number="1.9">
<h2 data-number="1.9" class="anchored" data-anchor-id="a-choose-your-own-adventure-moment"><span class="header-section-number">1.9</span> A <em>Choose Your Own Adventure</em> moment</h2>
<p>If you would like to learn more about how to use deep learning models in practice, including how to identify and fix errors, create a real working web application, and avoid your model causing unexpected harm to your organization or society more generally, then keep reading the next two chapters. If you would like to start learning the foundations of how deep learning works under the hood, skip to <a href="mnist_basics.html" class="quarto-xref"><span>Chapter 4</span></a>. (Did you ever read <em>Choose Your Own Adventure</em> books as a kid? Well, this is kind of like that… except with more deep learning than that book series contained.)</p>
<p>You will need to read all these chapters to progress further in the book, but it is totally up to you which order you read them in. They don’t depend on each other. If you skip ahead to <a href="mnist_basics.html" class="quarto-xref"><span>Chapter 4</span></a>, we will remind you at the end to come back and read the chapters you skipped over before you go any further.</p>
</section>
<section id="questionnaire" class="level2" data-number="1.10">
<h2 data-number="1.10" class="anchored" data-anchor-id="questionnaire"><span class="header-section-number">1.10</span> Questionnaire</h2>
<p>It can be hard to know in pages and pages of prose what the key things are that you really need to focus on and remember. So, we’ve prepared a list of questions and suggested steps to complete at the end of each chapter. All the answers are in the text of the chapter, so if you’re not sure about anything here, reread that part of the text and make sure you understand it. Answers to all these questions are also available on the <a href="https://book.fast.ai">book’s website</a>. You can also visit <a href="https://forums.fast.ai">the forums</a> if you get stuck to get help from other folks studying this material.</p>
<p>For more questions, including detailed answers and links to the video timeline, have a look at Radek Osmulski’s <a href="http://aiquizzes.com/howto">aiquizzes</a>.</p>
<ol type="1">
<li><p>Do you need these for deep learning?</p>
<ul>
<li>Lots of math T / F</li>
<li>Lots of data T / F</li>
<li>Lots of expensive computers T / F</li>
<li>A PhD T / F</li>
</ul></li>
<li><p>Name five areas where deep learning is now the best in the world.</p></li>
<li><p>What was the name of the first device that was based on the principle of the artificial neuron?</p></li>
<li><p>Based on the book of the same name, what are the requirements for parallel distributed processing (PDP)?</p></li>
<li><p>What were the two theoretical misunderstandings that held back the field of neural networks?</p></li>
<li><p>What is a GPU?</p></li>
<li><p>Open a notebook and execute a cell containing: <code>1+1</code>. What happens?</p></li>
<li><p>Follow through each cell of the stripped version of the notebook for this chapter. Before executing each cell, guess what will happen.</p></li>
<li><p>Complete the Jupyter Notebook online appendix.</p></li>
<li><p>Why is it hard to use a traditional computer program to recognize images in a photo?</p></li>
<li><p>What did Samuel mean by “weight assignment”?</p></li>
<li><p>What term do we normally use in deep learning for what Samuel called “weights”?</p></li>
<li><p>Draw a picture that summarizes Samuel’s view of a machine learning model.</p></li>
<li><p>Why is it hard to understand why a deep learning model makes a particular prediction?</p></li>
<li><p>What is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy?</p></li>
<li><p>What do you need in order to train a model?</p></li>
<li><p>How could a feedback loop impact the rollout of a predictive policing model?</p></li>
<li><p>Do we always have to use 224×224-pixel images with the cat recognition model?</p></li>
<li><p>What is the difference between classification and regression?</p></li>
<li><p>What is a validation set? What is a test set? Why do we need them?</p></li>
<li><p>What will fastai do if you don’t provide a validation set?</p></li>
<li><p>Can we always use a random sample for a validation set? Why or why not?</p></li>
<li><p>What is overfitting? Provide an example.</p></li>
<li><p>What is a metric? How does it differ from “loss”?</p></li>
<li><p>How can pretrained models help?</p></li>
<li><p>What is the “head” of a model?</p></li>
<li><p>What kinds of features do the early layers of a CNN find? How about the later layers?</p></li>
<li><p>Are image models only useful for photos?</p></li>
<li><p>What is an “architecture”?</p></li>
<li><p>What is segmentation?</p></li>
<li><p>What is <code>y_range</code> used for? When do we need it?</p></li>
<li><p>What are “hyperparameters”?</p></li>
<li><p>What’s the best way to avoid failures when using AI in an organization?</p></li>
</ol>
<section id="further-research" class="level3">
<h3 class="anchored" data-anchor-id="further-research">Further Research</h3>
<p>Each chapter also has a “Further Research” section that poses questions that aren’t fully answered in the text, or gives more advanced assignments. Answers to these questions aren’t on the book’s website; you’ll need to do your own research!</p>
<ol type="1">
<li>Why is a GPU useful for deep learning? How is a CPU different, and why is it less effective for deep learning?</li>
<li>Try to think of three areas where feedback loops might impact the use of machine learning. See if you can find documented examples of that happening in practice.</li>
</ol>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/fastai\.github\.io\/fastbook2e");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link" aria-label="Preface">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Preface</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./book2.html" class="pagination-link" aria-label="*From Model to Production*">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title"><em>From Model to Production</em></span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>